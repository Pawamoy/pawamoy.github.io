{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my website \u00a4 Below is the list of all my blog posts. You can also checkout my About page, or some projects I worked on in the Showcase .","title":"Home"},{"location":"#welcome-to-my-website","text":"Below is the list of all my blog posts. You can also checkout my About page, or some projects I worked on in the Showcase .","title":"Welcome to my website"},{"location":"about/","text":".post-header { display: none; } code { border: none; padding: 0; font-weight: bold; } p { margin-left: 40px !important; } p.i2 { display: none; } p.i2 + p { margin-left: 80px !important; } p a, p a:hover, p a:visited { text-decoration: underline; } ul { list-style-type: none; } NAME \u00a4 pawamoy \u2014 Timoth\u00e9e Mazzucotelli SYNOPSIS \u00a4 pawamoy [GLOBAL_OPTS] COMMAND [COMMAND_OPTS] [OBJECT | IDEA]... DESCRIPTION \u00a4 pawamoy lets you take control over the person identified by the name Timoth\u00e9e Mazzucotelli . Timoth\u00e9e Mazzucotelli was born in France and still lives there. He received education in Computer Science and obtained his Masters in C.S. in Universit\u00e9 de Strasbourg. He is therefore able to write code, amongst other things (see COMMANDS ). GLOBAL OPTIONS \u00a4 These options influence how the command will be run. Please note that some switches may not apply to some commands. --chatty Increase verbosity. This flag is automatically switched on when command drink is used with option --alcohol . Default: false. --enthusiast This option is always useful, especially when learning new things. Use it without caution! --fast Do things fast. Behavior becomes non-deterministic: things will be done well in average, but sometimes they will not. This option can yield more interesting results though. --happy Everyone wants to be happy right? Use this option regularly to ensure proper sanity of mind. --introvert Act with more reserve. Talk less. This option overrides and disables --chatty . There is a high probability that --over-thinking will be switched on using --introvert . This option can be used to avoid unnecessary jokes during professional interactions, but try not to use it too much as it will reduce enthusiasm and increase risk of switching --silent on. --open Stay open! Every thing you hear or see will be received with more curiosity and enthusiasm. Best used in combination with learn and work commands. --over-thinking Spend more time thinking about things than actually doing them. --perfectionist Nothing is ever good enough. Use this option to enable a \"constantly-improving-and-refactoring\" behavior. Beware: time spent in this mode is exponentially increased, and there is no guarantee to obtain a final product! --reluctant When inspiration is low, reluctant mode is activated. Things will be harder at first, but once the machinery is warmed-up, this mode will be deactivated and behavior will go back to normal. --safe Sometimes things are dangerous. Use this mode to increase cautiousness and avoid accidents. --silent Do not say a word and be silent. This mode can be used to silently sneak behind someone and surprise them. Be sure to know the person though. This mode is also used at night, to avoid waking up the significant other. --slow Side-effect of --reluctant , also triggered when tired. Everyone needs a bit of slowness from time to time right? COMMANDS \u00a4 code Write or design code. It implies thinking, and can imply drawing. This command can be run regularly, without moderation. It should not be used 100% of the time though, because other vital tasks need attention. drink Drink liquids. Options are: --water (the default), --juice , --alcohol and --soda . Juices are good in the morning, while alcohol is better for social events, though not mandatory. Soda is really an extra, for example when eating pizza (both go well together). Water is mandatory. A bottle of it must always be available at night. eat Eat food. Almost every kind of food is accepted as positional argument. Ability to eat insects is not yet implemented, but might be in the future. exercise Exercise command should be run regularly, like two or three times a week. Option --bike is built-in and very often used as the main transport mean. Currently, option --badminton is available, and soon maybe --basketball will be implemented. learn Learn new things. It takes time and depending on the thing to learn, --relucant might be enabled at first. In any case, don't forget to use the --open global option to ease the process. One thing that seems to be instantly learned and remembered is new keyboard shortcuts. listen Focus on listening. Probability of talking is decreased. This command is well used when someone is asking a question. It helps preventing an anticipation bias that often leads to an incorrect comprehension of the question, and thus an incorrect answer. love You can love many things and people. Don't restrict yourself. Isn't love a choice after all? Brutal black metal was not loved at first, it took a bit of time and training to be able to listen to it, but now it's one of the most-cherished thing! play Play single or multi-player. Card games, video games, especially horror ones, all sorts of games! Option --vr is already implemented but is waiting for adequate hardware. read Read a book, an article, on joke, docs, a tutorial, news. Read it on a desktop screen, on a smartphone screen, on an e-book screen, or... on paper directly? Technology is crazy. While using this option, please take care of your eyes: enable blue-light filters and adapt luminosity. sleep Sleep must be done regularly and in sufficient quantity to ensure proper performance in any other activity. Run this command at least once per day (better during the night) for at least eight hours. More is better. Less, and headache will appear, and --reluctant flag will be turned on until more sleep is done. talk Talk about universal determinism at parties. think Don't start to write code for your complex project idea immediately! Think it before! But be careful: not too much . You'll want to implement all the options. Yes, all. It will end as a generic program to do anything, and it will fail. work Chop chop! It's time to work! But your work is your passion, so it's not really work, is it? write Write man pages, blog posts, documentation, code, fiction. You should write more fiction. BUGS \u00a4 pawamoy has an extra pair of ribs. This bug does not come from the two engineers that designed pawamoy . It is due to some binary data corruption during replication over network. It can't be fixed. COPYRIGHT \u00a4 Copyright 1991-2009 Mazzucotelli Foundation, Inc. This is proprietary software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. ENVIRONMENT VARIABLES \u00a4 HOME - Define the home pawamoy will use. \"Moon\", \"Area 51\", or \"Public bathrooms\" are not valid values. ENERGY - An integer between 1 and 100. Using 0 will cause pawamoy to self-destruct. Use with caution. Use commands eat then sleep when ENERGY is low to increase it again. HEALTH - An integer between 1 and 100. Using 0 is reserved for the end-of-life date of pawamoy . Don't use it before. HUNGER - An integer between 1 and 100. 0 mens extra-full, 100 means starving. Use command eat to decrease HUNGER . TEMPERATURE - In degree Celsius. Try to increase it just a bit when HEALTH is lowering. This is experimental. Use it at my own risk. FILES \u00a4 /boot/config - This file was useful only once, because pawamoy can never reboot. Once shut down, it stays shut down. /sys/cpu - This file was auto-generated, and is self-mutating. Please don't mess too much with this file. /etc/pawamoy/principles - You can modify principles here. Don't add too many. /var/log/pawamoy - Interesting statistics and analytics about pawamoy usage. LICENSE \u00a4 pawamoy is released under the terms of the Human Decency license. Please use it accordingly. You cannot duplicate pawamoy . At least for now. SEE ALSO \u00a4 @pawamoy(dev.to) , @pawamoy(github) , @paaawamoy(instagram) , @pawamoy(reddit) , @pawamoy(stackoverflow) , @pawamoy(twitter) NOTES \u00a4 This man page is a perpetual work in progress. Expect some delays between pawamoy releases and its documentation updates.","title":"About"},{"location":"about/#name","text":"pawamoy \u2014 Timoth\u00e9e Mazzucotelli","title":"NAME"},{"location":"about/#synopsis","text":"pawamoy [GLOBAL_OPTS] COMMAND [COMMAND_OPTS] [OBJECT | IDEA]...","title":"SYNOPSIS"},{"location":"about/#description","text":"pawamoy lets you take control over the person identified by the name Timoth\u00e9e Mazzucotelli . Timoth\u00e9e Mazzucotelli was born in France and still lives there. He received education in Computer Science and obtained his Masters in C.S. in Universit\u00e9 de Strasbourg. He is therefore able to write code, amongst other things (see COMMANDS ).","title":"DESCRIPTION"},{"location":"about/#global-options","text":"These options influence how the command will be run. Please note that some switches may not apply to some commands. --chatty Increase verbosity. This flag is automatically switched on when command drink is used with option --alcohol . Default: false. --enthusiast This option is always useful, especially when learning new things. Use it without caution! --fast Do things fast. Behavior becomes non-deterministic: things will be done well in average, but sometimes they will not. This option can yield more interesting results though. --happy Everyone wants to be happy right? Use this option regularly to ensure proper sanity of mind. --introvert Act with more reserve. Talk less. This option overrides and disables --chatty . There is a high probability that --over-thinking will be switched on using --introvert . This option can be used to avoid unnecessary jokes during professional interactions, but try not to use it too much as it will reduce enthusiasm and increase risk of switching --silent on. --open Stay open! Every thing you hear or see will be received with more curiosity and enthusiasm. Best used in combination with learn and work commands. --over-thinking Spend more time thinking about things than actually doing them. --perfectionist Nothing is ever good enough. Use this option to enable a \"constantly-improving-and-refactoring\" behavior. Beware: time spent in this mode is exponentially increased, and there is no guarantee to obtain a final product! --reluctant When inspiration is low, reluctant mode is activated. Things will be harder at first, but once the machinery is warmed-up, this mode will be deactivated and behavior will go back to normal. --safe Sometimes things are dangerous. Use this mode to increase cautiousness and avoid accidents. --silent Do not say a word and be silent. This mode can be used to silently sneak behind someone and surprise them. Be sure to know the person though. This mode is also used at night, to avoid waking up the significant other. --slow Side-effect of --reluctant , also triggered when tired. Everyone needs a bit of slowness from time to time right?","title":"GLOBAL OPTIONS"},{"location":"about/#commands","text":"code Write or design code. It implies thinking, and can imply drawing. This command can be run regularly, without moderation. It should not be used 100% of the time though, because other vital tasks need attention. drink Drink liquids. Options are: --water (the default), --juice , --alcohol and --soda . Juices are good in the morning, while alcohol is better for social events, though not mandatory. Soda is really an extra, for example when eating pizza (both go well together). Water is mandatory. A bottle of it must always be available at night. eat Eat food. Almost every kind of food is accepted as positional argument. Ability to eat insects is not yet implemented, but might be in the future. exercise Exercise command should be run regularly, like two or three times a week. Option --bike is built-in and very often used as the main transport mean. Currently, option --badminton is available, and soon maybe --basketball will be implemented. learn Learn new things. It takes time and depending on the thing to learn, --relucant might be enabled at first. In any case, don't forget to use the --open global option to ease the process. One thing that seems to be instantly learned and remembered is new keyboard shortcuts. listen Focus on listening. Probability of talking is decreased. This command is well used when someone is asking a question. It helps preventing an anticipation bias that often leads to an incorrect comprehension of the question, and thus an incorrect answer. love You can love many things and people. Don't restrict yourself. Isn't love a choice after all? Brutal black metal was not loved at first, it took a bit of time and training to be able to listen to it, but now it's one of the most-cherished thing! play Play single or multi-player. Card games, video games, especially horror ones, all sorts of games! Option --vr is already implemented but is waiting for adequate hardware. read Read a book, an article, on joke, docs, a tutorial, news. Read it on a desktop screen, on a smartphone screen, on an e-book screen, or... on paper directly? Technology is crazy. While using this option, please take care of your eyes: enable blue-light filters and adapt luminosity. sleep Sleep must be done regularly and in sufficient quantity to ensure proper performance in any other activity. Run this command at least once per day (better during the night) for at least eight hours. More is better. Less, and headache will appear, and --reluctant flag will be turned on until more sleep is done. talk Talk about universal determinism at parties. think Don't start to write code for your complex project idea immediately! Think it before! But be careful: not too much . You'll want to implement all the options. Yes, all. It will end as a generic program to do anything, and it will fail. work Chop chop! It's time to work! But your work is your passion, so it's not really work, is it? write Write man pages, blog posts, documentation, code, fiction. You should write more fiction.","title":"COMMANDS"},{"location":"about/#bugs","text":"pawamoy has an extra pair of ribs. This bug does not come from the two engineers that designed pawamoy . It is due to some binary data corruption during replication over network. It can't be fixed.","title":"BUGS"},{"location":"about/#copyright","text":"Copyright 1991-2009 Mazzucotelli Foundation, Inc. This is proprietary software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.","title":"COPYRIGHT"},{"location":"about/#environment-variables","text":"HOME - Define the home pawamoy will use. \"Moon\", \"Area 51\", or \"Public bathrooms\" are not valid values. ENERGY - An integer between 1 and 100. Using 0 will cause pawamoy to self-destruct. Use with caution. Use commands eat then sleep when ENERGY is low to increase it again. HEALTH - An integer between 1 and 100. Using 0 is reserved for the end-of-life date of pawamoy . Don't use it before. HUNGER - An integer between 1 and 100. 0 mens extra-full, 100 means starving. Use command eat to decrease HUNGER . TEMPERATURE - In degree Celsius. Try to increase it just a bit when HEALTH is lowering. This is experimental. Use it at my own risk.","title":"ENVIRONMENT VARIABLES"},{"location":"about/#files","text":"/boot/config - This file was useful only once, because pawamoy can never reboot. Once shut down, it stays shut down. /sys/cpu - This file was auto-generated, and is self-mutating. Please don't mess too much with this file. /etc/pawamoy/principles - You can modify principles here. Don't add too many. /var/log/pawamoy - Interesting statistics and analytics about pawamoy usage.","title":"FILES"},{"location":"about/#license","text":"pawamoy is released under the terms of the Human Decency license. Please use it accordingly. You cannot duplicate pawamoy . At least for now.","title":"LICENSE"},{"location":"about/#see-also","text":"@pawamoy(dev.to) , @pawamoy(github) , @paaawamoy(instagram) , @pawamoy(reddit) , @pawamoy(stackoverflow) , @pawamoy(twitter)","title":"SEE ALSO"},{"location":"about/#notes","text":"This man page is a perpetual work in progress. Expect some delays between pawamoy releases and its documentation updates.","title":"NOTES"},{"location":"credits/","text":"I want to say thank you to all these projects, companies, foundations, consortiums and alliances on which I stood to create things! And another gigantic THANK YOU to the Open Source community: authors, contributors, maintainers, sustainers, bloggers, writers, testers, backers, and the rest! This blog is graciously hosted by GitHub , and is built with MkDocs and the Material for Mkdocs theme.","title":"Credits"},{"location":"showcase/","text":"","title":"Showcase Grid"},{"location":"drafts/2018-11-27-ssh-keys-and-passwords-management/","text":"When you are given access to a lot of remote servers, it's getting more and more complicated to remember the passwords for every SSH key and server account. In this post I will show you a method to organize your SSH keys and passwords in a secure manner. The goal of this method is to deal with: SSH keys their associated passwords remote server accounts login and passwords SSH keys \u00a4 I will distinguish the different usages we make of SSH keys. Personal machine to remote server SSH key: let's call it a P-R key , like Personal-to-Remote . It's a key you use to connect to a server from your laptop or from your work machine. Remote server to remote server SSH key: a R-R key , like Remote-to-Remote . It's a key present on a remote server, used to connect to another remote server. Remote server to remote server SSH key: a R-R key , like Remote-to-Remote . It's a key present on a remote server, used to connect to another remote server.","title":"Managing SSH keys for remote servers and services: a solution with tomb and pass"},{"location":"drafts/2018-11-27-ssh-keys-and-passwords-management/#ssh-keys","text":"I will distinguish the different usages we make of SSH keys. Personal machine to remote server SSH key: let's call it a P-R key , like Personal-to-Remote . It's a key you use to connect to a server from your laptop or from your work machine. Remote server to remote server SSH key: a R-R key , like Remote-to-Remote . It's a key present on a remote server, used to connect to another remote server. Remote server to remote server SSH key: a R-R key , like Remote-to-Remote . It's a key present on a remote server, used to connect to another remote server.","title":"SSH keys"},{"location":"notes/install_printer_on_linux/","text":"Install a printer on Linux \u00a4 If system-config-printer does not work, you will need to do it via CUPS web interface. Go to http://localhost:631/admin , click on \"Add printer\". It will ask for a username and password. Try \"root\" and your \"root\" password. If is does not work, then edit the file \"/etc/cups/cupsd.conf\". Instead of \"Basic\", put \"None\" at \"DefaultAuthType\". Also remove every line containing \"@SYSTEM\". Restart CUPS with sudo service cups restart . Go again on the web interface, click on \"Add printer\". Download your printer's driver at www.support.xerox.com by choosing the Generic PPD. It's an executable file (.exe) that you can open with your archive manager on Linux. Extract the PPD file and use it in CUPS. Should be good now, except it's considered a generic printer and will mess up your with your fonts. Great. https://www.linuxquestions.org/questions/linux-software-2/cups-username-and-password-156986/ http://forum.support.xerox.com/t5/Printing/CUPS-Driver-PPD-file-for-Xerox-3345-WorkCentre-printer-for-Linux/td-p/203672 https://github.com/zdohnal/system-config-printer/issues/36","title":"Install a printer on Linux"},{"location":"notes/install_printer_on_linux/#install-a-printer-on-linux","text":"If system-config-printer does not work, you will need to do it via CUPS web interface. Go to http://localhost:631/admin , click on \"Add printer\". It will ask for a username and password. Try \"root\" and your \"root\" password. If is does not work, then edit the file \"/etc/cups/cupsd.conf\". Instead of \"Basic\", put \"None\" at \"DefaultAuthType\". Also remove every line containing \"@SYSTEM\". Restart CUPS with sudo service cups restart . Go again on the web interface, click on \"Add printer\". Download your printer's driver at www.support.xerox.com by choosing the Generic PPD. It's an executable file (.exe) that you can open with your archive manager on Linux. Extract the PPD file and use it in CUPS. Should be good now, except it's considered a generic printer and will mess up your with your fonts. Great. https://www.linuxquestions.org/questions/linux-software-2/cups-username-and-password-156986/ http://forum.support.xerox.com/t5/Printing/CUPS-Driver-PPD-file-for-Xerox-3345-WorkCentre-printer-for-Linux/td-p/203672 https://github.com/zdohnal/system-config-printer/issues/36","title":"Install a printer on Linux"},{"location":"notes/python_best_practives/","text":"From quantified-code/python-anti-patterns on GitHub \u00a4 Use named tuples when returning more than one value from a function Raise an error instead of returning None which is causing multiple return types Think EAFP, Easier to Ask for Forgiveness than Permission: assume file exists, catch OSError if not, don't verify Use defaultdict instead of using condition to set item to a base value if it does not exist Use explicit unpacking, don't access tuple elements by index","title":"From quantified-code/python-anti-patterns on GitHub"},{"location":"notes/python_best_practives/#from-quantified-codepython-anti-patterns-on-github","text":"Use named tuples when returning more than one value from a function Raise an error instead of returning None which is causing multiple return types Think EAFP, Easier to Ask for Forgiveness than Permission: assume file exists, catch OSError if not, don't verify Use defaultdict instead of using condition to set item to a base value if it does not exist Use explicit unpacking, don't access tuple elements by index","title":"From quantified-code/python-anti-patterns on GitHub"},{"location":"notes/ssh_management/","text":"Use the diagram","title":"Ssh management"},{"location":"posts/challenge-fill-space-one-line-90-degree-same-direction/","text":"Last night I was doing that in my head: filling a 2D space with one continuous line doing turns at a 90\u00b0 angle, always in the same direction. Other people do this as well, right? Well, today I actually drawn it: I started top left, and finish bottom right. I turned right all the time. It took me about 20 minutes. It was not so easy! The difficult parts are: - keeping the line straightly vertical or horizontal - not getting stuck (because we don't want to make the line touch) - not repeating the same patterns too much (though repetition of symmetrical patterns could be very pretty!) Here are some mistakes I made (the most obvious ones): But anyway, it was my first attempt, and I kinda like the childish aspect of it. 10/10 would do again! Show me your attempts in the comments ! Note: I used a cotton canvas which cost me 1,50\u20ac, but you can just use a sheet of paper if you don't plan to show how good you are to everybody by hanging it on a wall.","title":"Challenge: fill a 2D space with one continuous line doing 90\u00b0 angle turns in the same direction (left/right)"},{"location":"posts/django-auth-server-for-shiny/","text":"As you may know, Shiny Server comes in two versions: open-source and professional. The professional adds security and authentication features like password protected applications , and controlled access via SSL and LDAP, Active Directory, Google OAuth, PAM, proxied authentication, or passwords . If you need these authentication features but don't want or can't spend $9,995 per year for the professional edition, then I got a solution for you! In this post, I will show how to wrap one or several Shiny applications into a Django application to add authentication and access control to your Shiny apps. The shining star here will not be Django, as you could replace it by any other web application you want, but the famous NginX reverse proxy, accompanied by its devoted auth-request module. The code used in this post is also available as a repository on GitHub . It contains a Docker configuration so you can try it easily. On the menu Overview Wrapping a Shiny app into a Django-powered page Proxying Shiny requests to the Shiny app Adding an authentication step for every Shiny request Try it with a Dockerized project Comments What most of you will be interested in is starting at section 2: Proxying , though Wrapping is interesting as well if you want to create an interface to access multiple Shiny apps. Overview \u00a4 Let's look at some pictures to see what we want to accomplish. The first picture shows our client-server architecture. The client can communicate with the server on the port 80 or 443 (HTTP or HTTPs), but not on the ports 8000 or 8100, which are used internally by Django and Shiny. This can be configured through the firewall. The second picture shows what happens when the client requests the URL that the Shiny app is served on. As we wrap the Shiny app into a Django-powered page, the request is proxied directly to Django by NginX. Django then gets the initial Shiny page HTML contents with an internal HTTP Get request, and renders it in a iframe (we will see the details later). It then returns this rendered page to NginX, which returns it to the client. The third picture shows each subsequent requests from the client to the server through a WebSocket, and how NginX is asking authorization to Django. When NginX receives the request, it sends a sub-request to Django, asking for permission to proxy the request to Shiny and return the response to the client. If Django says yes (HTTP 200), NginX proxies the request to Shiny. If Django says no (HTTP 403 or any other error code), NginX rejects the request by returning HTTP 403 as a response to the client. OK, let's try it! To begin, create a directory that we will use for this tutorial: mkdir django-shiny cd django-shiny Wrapping a Shiny app into a Django-powered page \u00a4 This first section will help you setup an example project to follow this tutorial, but the first two steps described are optional. You can immediately jump to the Proxying Shiny requests section where we will a use pre-setup example project using Docker. You will need to install Docker if you don't already have it. See Install Docker for installation instructions. The third step however might be interesting to read if you need to wrap your Shiny app into a Django-powered page and website, instead of just using Django as an external authentication / authorization backend. The Shiny app The Django app Injecting the HTML contents in an Iframe The Shiny app \u00a4 Let's get a Shiny app example from RStudio's gallery . The code is available on GitHub in this repository . Clone it in a sub-directory called shinyapp : git clone --depth = 1 https://github.com/rstudio/shiny-examples mv shiny-examples/001-hello shinyapp rm -rf shiny-examples We also need to install the Shiny R package. If you don't already have R installed, you can install a recent version with the following commands: sudo add-apt-repository \"deb http://cran.rstudio.com/bin/linux/ubuntu trusty/\" sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9 sudo add-apt-repository ppa:marutter/rdev sudo apt-get update sudo apt-get install -y r-base Run this command to install Shiny: sudo R -e \"install.packages('shiny', repos='https://cran.rstudio.com/')\" To run the Shiny application on port 8100, use the following command: sudo R -e \"shiny::runApp(appDir='shinyapp', port=8100)\" Try to go to http://localhost:8100 to see if the app is running. The Django app \u00a4 We will create a new Django project called djangoapp . If you don't already have Django installed on your system, install it in a virtualenv with pip install Django , or system-wide with sudo pip install Django . To create the project, run the following command: django-admin startproject djangoapp We need to initialize the SQLite database first. python djangoapp/manage.py migrate You can now run the Django application on port 8000 with the following command: python djangoapp/manage.py runserver localhost:8000 And try to go to http://localhost:8000 to see if the app is running. Injecting the HTML contents in an Iframe \u00a4 At this point you should have the following tree: . \u251c\u2500\u2500 djangoapp \u2502 \u251c\u2500\u2500 db.sqlite3 \u2502 \u251c\u2500\u2500 djangoapp \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 settings.py \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2514\u2500\u2500 wsgi.py \u2502 \u2514\u2500\u2500 manage.py \u2514\u2500\u2500 shinyapp \u251c\u2500\u2500 app.R \u251c\u2500\u2500 DESCRIPTION \u2514\u2500\u2500 Readme.md 3 directories, 9 files We will proceed in three main steps: Create a simple view that renders our wrapping HTML page Create this wrapping HTML page that will: Create an Iframe and add it to the DOM Get the Shiny HTML contents in JSON format, with an asynchronous call Inject these contents in the Iframe Create the view to return Shiny HTML contents as JSON by: Getting the contents with a GET request Parsing it with BeautifulSoup 's HTML parser Dumping and returning it as JSON Create the wrapping view \u00a4 Let's create our wrapping view. First make sure you have listed djangoapp in the Django settings' INSTALLED_APPS : # settings.py INSTALLED_APPS = [ 'django.contrib.admin' , 'django.contrib.auth' , 'django.contrib.contenttypes' , 'django.contrib.sessions' , 'django.contrib.messages' , 'django.contrib.staticfiles' , 'djangoapp' ] Then we can add the URL in urls.py : from django.contrib import admin from django.urls import path from . import views urlpatterns = [ path ( 'admin/' , admin . site . urls ), path ( 'shiny/' , views . shiny , name = 'shiny' ), ] And now we create the view in a new views.py file: touch djangoapp/djangoapp/views.py from django.shortcuts import render , redirect def shiny ( request ): return render ( request , 'djangoapp/shiny.html' ) Create the wrapping HTML page \u00a4 Since we tell the view to render the djangoapp/shiny.html template, we need to create it: mkdir -p djangoapp/djangoapp/templates/djangoapp touch djangoapp/djangoapp/templates/djangoapp/shiny.html # too much of djangoapp already, I know And write its contents. We simply add a title, to know we are in the wrapping view, and then we add a script to retrieve the Shiny app contents: < h1 > We are in the wrapping page! </ h1 > < div id = \"contents\" ></ div > < script src = \"https://code.jquery.com/jquery-3.3.1.min.js\" ></ script > < script > $ ( document ). ready ( function () { $ . getJSON ( '{% raw %}{% url \"shiny_contents\" %}{% endraw %}' , function ( data ) { var iframe = document . createElement ( \"iframe\" ); $ ( '#contents' ). append ( iframe ); iframe . contentWindow . document . open (); iframe . contentWindow . document . write ( data . html_contents ); iframe . contentWindow . document . close (); // Attempt circumvention if ( iframe . contentWindow . WebSocket ) WebSocket = iframe . contentWindow . WebSocket ; }); }); </ script > Several things happen here: - we declare a div of ID contents in which we will add an iframe , - we make use of JQuery's $(document).ready and $.getJSON methods to load HTML contents from an URL returning JSON, - we create the iframe , add it in the document, then write the HTML contents inside of it. We also reassign the WebSocket variable to the value of the iframe one. Create the JSON view \u00a4 As you maybe guessed, work is not finished. We need to add the shiny_contents URL and view in the Django app. The view must return the contents of the Shiny app initial page as JSON. Add the URL in urls.py : from django.contrib import admin from django.urls import path from . import views urlpatterns = [ path ( 'admin/' , admin . site . urls ), path ( 'shiny/' , views . shiny , name = 'shiny' ), path ( 'shiny_contents/' , views . shiny_contents , name = 'shiny_contents' ), ] Add the view to views.py : from django.http import JsonResponse from django.shortcuts import render import requests from bs4 import BeautifulSoup def shiny ( request ): return render ( request , 'djangoapp/shiny.html' ) def shiny_contents ( request ): response = requests . get ( 'http://localhost:8100' ) soup = BeautifulSoup ( response . content , 'html.parser' ) return JsonResponse ({ 'html_contents' : str ( soup )}) We are using requests and BeautifulSoup to get the HTML contents and return it as text, dumped as JSON. If you know a better way, let me know in the comments! To install requests and BeautifulSoup: pip install requests beautifulsoup4 . OK, let's try! Run both the Shiny application and the Django application, then go to http://localhost:8000/shiny to see how it goes! You should see something like the following image: But since Shiny requests are not proxied, they are simply lost in-between, and your Shiny app will not respond, nor get a nice look because the static assets will not be loaded. We will fix this in the next section: Proxying Shiny requests . This is it for the setup and the Django-wrapped Shiny page. For the rest of the tutorial, I will explain how to configure NginX and Django to act as proxy and authorization servers. The result is available for you to try in this repository: https://github.com/Pawamoy/docker-nginx-auth-request-django-shiny-example . See the Try it out section. Proxying Shiny requests to the Shiny app \u00a4 This is time for NginX to come on stage. We need to create our new, project-specific configuration file in /etc/nginx/sites-available/djangoshiny . We will tell NginX to proxy every request with an URL like shiny/* to our Shiny app. All other requests will be proxied to the Django app. # declare your Django app upstream djangoapp_server { server localhost : 8000 ; } # declare your Shiny app upstream shinyapp_server { server localhost : 8100 ; } # required for WebSockets map $http_upgrade $connection_upgrade { default upgrade ; '' close ; } server { listen 80 ; server_name localhost ; client_max_body_size 100M ; # normal requests go to Django location / { proxy_pass http://djangoapp_server ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_set_header Host $host ; proxy_redirect off ; if (!-f $request_filename ) { proxy_pass http://djangoapp_server ; break ; } } # \"shiny\" requests go to Shiny location ~ /shiny/.+ { rewrite ^/shiny/(.*) $ / $1 break ; proxy_pass http://shinyapp_server ; proxy_redirect http://shinyapp_server/ $scheme://$host/shiny/ ; # required for WebSockets proxy_http_version 1 .1 ; proxy_set_header Upgrade $http_upgrade ; proxy_set_header Connection $connection_upgrade ; proxy_read_timeout 20d ; proxy_buffering off ; } } Once you're ready, enable this configuration by linking it like this: sudo ln -s /etc/nginx/sites-available/djangoshiny /etc/nginx/sites-enabled/djangoshiny Now reload NginX, and that's it! If you launch both Django on port 8000, and Shiny on port 8100, you should be able to connect to http://localhost/shiny and enjoy your Django-wrapped Shiny application functioning properly! There is still one thing we need to add: authentication and authorization. I won't show how to create sign-in / sign-up / sign-out views: you can find all the documentation to enable Django authentication system online. What I will show you is how to make Django act as an authorization server for Shiny. Adding an authentication step for every Shiny request \u00a4 We want all requests proxied to Shiny to be authorized by Django. For this we are gonna use the auth-request module. This module is not compiled in NginX by default on distributions like Ubuntu / Debian. If you want to recompile NginX with auth-request enabled, check how I do it in this Dockerfile . An easier solution is to use the Docker setup from the same repository, with the official NginX image which already supports auth-request. So, once your NginX is ready, add this authorization step in the configuration file: location ~ /shiny/.+ { # we tell nginx to call that location for each request auth_request /auth ; rewrite ^/shiny/(.*) $ / $1 break ; proxy_pass http://shinyapp_server ; proxy_redirect http://shinyapp_server/ $scheme://$host/shiny/ ; # this part is needed for WebSockets to work proxy_http_version 1 .1 ; proxy_set_header Upgrade $http_upgrade ; proxy_set_header Connection $connection_upgrade ; proxy_read_timeout 20d ; proxy_buffering off ; } # the /auth location will send a subrequest to django, URL shiny_auth/ location = /auth { internal ; proxy_pass http://localhost:8000/shiny_auth/ ; proxy_pass_request_body off ; proxy_set_header Content-Length \"\" ; proxy_set_header X-Original-URI $request_uri ; } Of course, we also need to write the view called by the shiny_auth/ URL. It's a very simple one. First add the URL in urls.py : from django.contrib import admin from django.urls import path from . import views urlpatterns = [ path ( 'admin/' , admin . site . urls ), path ( 'shiny/' , views . shiny , name = 'shiny' ), path ( 'shiny_contents/' , views . shiny_contents , name = 'shiny_contents' ), path ( 'shiny_auth/' , views . shiny_auth ), ] And then the view in views.py : from django.http import HttpResponse def shiny_auth ( request ): if request . user . is_authenticated : return HttpResponse ( status = 200 ) return HttpResponse ( status = 403 ) Et voil\u00e0! If the user is authenticated, Django will say \"OK\". If the user is not authenticated, Django will say \"No\". But you could implement any logic you need instead of just checking if the user is authenticated or not. You could have several Shiny apps served on different URLs, powered by Django (in wrapping pages, as we saw earlier), and grant access to users with a permission system, etc.. Try it with a Dockerized project \u00a4 Clone the repository and build the images with the following commands: git clone https://github.com/Pawamoy/docker-nginx-auth-request-django-shiny-example docker-django-shiny cd docker-django-shiny sudo make all This first build should take quite some time. It will build Shiny and Django images, get static assets, create the database, create a super user, and then run the application. You will then be able to go to http://localhost:8000 to see it in action. To print the available make commands, simply run make .","title":"Django application as an authentication / authorization server for Shiny"},{"location":"posts/django-auth-server-for-shiny/#overview","text":"Let's look at some pictures to see what we want to accomplish. The first picture shows our client-server architecture. The client can communicate with the server on the port 80 or 443 (HTTP or HTTPs), but not on the ports 8000 or 8100, which are used internally by Django and Shiny. This can be configured through the firewall. The second picture shows what happens when the client requests the URL that the Shiny app is served on. As we wrap the Shiny app into a Django-powered page, the request is proxied directly to Django by NginX. Django then gets the initial Shiny page HTML contents with an internal HTTP Get request, and renders it in a iframe (we will see the details later). It then returns this rendered page to NginX, which returns it to the client. The third picture shows each subsequent requests from the client to the server through a WebSocket, and how NginX is asking authorization to Django. When NginX receives the request, it sends a sub-request to Django, asking for permission to proxy the request to Shiny and return the response to the client. If Django says yes (HTTP 200), NginX proxies the request to Shiny. If Django says no (HTTP 403 or any other error code), NginX rejects the request by returning HTTP 403 as a response to the client. OK, let's try it! To begin, create a directory that we will use for this tutorial: mkdir django-shiny cd django-shiny","title":"Overview"},{"location":"posts/django-auth-server-for-shiny/#wrapping-a-shiny-app-into-a-django-powered-page","text":"This first section will help you setup an example project to follow this tutorial, but the first two steps described are optional. You can immediately jump to the Proxying Shiny requests section where we will a use pre-setup example project using Docker. You will need to install Docker if you don't already have it. See Install Docker for installation instructions. The third step however might be interesting to read if you need to wrap your Shiny app into a Django-powered page and website, instead of just using Django as an external authentication / authorization backend. The Shiny app The Django app Injecting the HTML contents in an Iframe","title":"Wrapping a Shiny app into a Django-powered page"},{"location":"posts/django-auth-server-for-shiny/#the-shiny-app","text":"Let's get a Shiny app example from RStudio's gallery . The code is available on GitHub in this repository . Clone it in a sub-directory called shinyapp : git clone --depth = 1 https://github.com/rstudio/shiny-examples mv shiny-examples/001-hello shinyapp rm -rf shiny-examples We also need to install the Shiny R package. If you don't already have R installed, you can install a recent version with the following commands: sudo add-apt-repository \"deb http://cran.rstudio.com/bin/linux/ubuntu trusty/\" sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9 sudo add-apt-repository ppa:marutter/rdev sudo apt-get update sudo apt-get install -y r-base Run this command to install Shiny: sudo R -e \"install.packages('shiny', repos='https://cran.rstudio.com/')\" To run the Shiny application on port 8100, use the following command: sudo R -e \"shiny::runApp(appDir='shinyapp', port=8100)\" Try to go to http://localhost:8100 to see if the app is running.","title":"The Shiny app"},{"location":"posts/django-auth-server-for-shiny/#the-django-app","text":"We will create a new Django project called djangoapp . If you don't already have Django installed on your system, install it in a virtualenv with pip install Django , or system-wide with sudo pip install Django . To create the project, run the following command: django-admin startproject djangoapp We need to initialize the SQLite database first. python djangoapp/manage.py migrate You can now run the Django application on port 8000 with the following command: python djangoapp/manage.py runserver localhost:8000 And try to go to http://localhost:8000 to see if the app is running.","title":"The Django app"},{"location":"posts/django-auth-server-for-shiny/#injecting-the-html-contents-in-an-iframe","text":"At this point you should have the following tree: . \u251c\u2500\u2500 djangoapp \u2502 \u251c\u2500\u2500 db.sqlite3 \u2502 \u251c\u2500\u2500 djangoapp \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 settings.py \u2502 \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2502 \u2514\u2500\u2500 wsgi.py \u2502 \u2514\u2500\u2500 manage.py \u2514\u2500\u2500 shinyapp \u251c\u2500\u2500 app.R \u251c\u2500\u2500 DESCRIPTION \u2514\u2500\u2500 Readme.md 3 directories, 9 files We will proceed in three main steps: Create a simple view that renders our wrapping HTML page Create this wrapping HTML page that will: Create an Iframe and add it to the DOM Get the Shiny HTML contents in JSON format, with an asynchronous call Inject these contents in the Iframe Create the view to return Shiny HTML contents as JSON by: Getting the contents with a GET request Parsing it with BeautifulSoup 's HTML parser Dumping and returning it as JSON","title":"Injecting the HTML contents in an Iframe"},{"location":"posts/django-auth-server-for-shiny/#create-the-wrapping-view","text":"Let's create our wrapping view. First make sure you have listed djangoapp in the Django settings' INSTALLED_APPS : # settings.py INSTALLED_APPS = [ 'django.contrib.admin' , 'django.contrib.auth' , 'django.contrib.contenttypes' , 'django.contrib.sessions' , 'django.contrib.messages' , 'django.contrib.staticfiles' , 'djangoapp' ] Then we can add the URL in urls.py : from django.contrib import admin from django.urls import path from . import views urlpatterns = [ path ( 'admin/' , admin . site . urls ), path ( 'shiny/' , views . shiny , name = 'shiny' ), ] And now we create the view in a new views.py file: touch djangoapp/djangoapp/views.py from django.shortcuts import render , redirect def shiny ( request ): return render ( request , 'djangoapp/shiny.html' )","title":"Create the wrapping view"},{"location":"posts/django-auth-server-for-shiny/#create-the-wrapping-html-page","text":"Since we tell the view to render the djangoapp/shiny.html template, we need to create it: mkdir -p djangoapp/djangoapp/templates/djangoapp touch djangoapp/djangoapp/templates/djangoapp/shiny.html # too much of djangoapp already, I know And write its contents. We simply add a title, to know we are in the wrapping view, and then we add a script to retrieve the Shiny app contents: < h1 > We are in the wrapping page! </ h1 > < div id = \"contents\" ></ div > < script src = \"https://code.jquery.com/jquery-3.3.1.min.js\" ></ script > < script > $ ( document ). ready ( function () { $ . getJSON ( '{% raw %}{% url \"shiny_contents\" %}{% endraw %}' , function ( data ) { var iframe = document . createElement ( \"iframe\" ); $ ( '#contents' ). append ( iframe ); iframe . contentWindow . document . open (); iframe . contentWindow . document . write ( data . html_contents ); iframe . contentWindow . document . close (); // Attempt circumvention if ( iframe . contentWindow . WebSocket ) WebSocket = iframe . contentWindow . WebSocket ; }); }); </ script > Several things happen here: - we declare a div of ID contents in which we will add an iframe , - we make use of JQuery's $(document).ready and $.getJSON methods to load HTML contents from an URL returning JSON, - we create the iframe , add it in the document, then write the HTML contents inside of it. We also reassign the WebSocket variable to the value of the iframe one.","title":"Create the wrapping HTML page"},{"location":"posts/django-auth-server-for-shiny/#create-the-json-view","text":"As you maybe guessed, work is not finished. We need to add the shiny_contents URL and view in the Django app. The view must return the contents of the Shiny app initial page as JSON. Add the URL in urls.py : from django.contrib import admin from django.urls import path from . import views urlpatterns = [ path ( 'admin/' , admin . site . urls ), path ( 'shiny/' , views . shiny , name = 'shiny' ), path ( 'shiny_contents/' , views . shiny_contents , name = 'shiny_contents' ), ] Add the view to views.py : from django.http import JsonResponse from django.shortcuts import render import requests from bs4 import BeautifulSoup def shiny ( request ): return render ( request , 'djangoapp/shiny.html' ) def shiny_contents ( request ): response = requests . get ( 'http://localhost:8100' ) soup = BeautifulSoup ( response . content , 'html.parser' ) return JsonResponse ({ 'html_contents' : str ( soup )}) We are using requests and BeautifulSoup to get the HTML contents and return it as text, dumped as JSON. If you know a better way, let me know in the comments! To install requests and BeautifulSoup: pip install requests beautifulsoup4 . OK, let's try! Run both the Shiny application and the Django application, then go to http://localhost:8000/shiny to see how it goes! You should see something like the following image: But since Shiny requests are not proxied, they are simply lost in-between, and your Shiny app will not respond, nor get a nice look because the static assets will not be loaded. We will fix this in the next section: Proxying Shiny requests . This is it for the setup and the Django-wrapped Shiny page. For the rest of the tutorial, I will explain how to configure NginX and Django to act as proxy and authorization servers. The result is available for you to try in this repository: https://github.com/Pawamoy/docker-nginx-auth-request-django-shiny-example . See the Try it out section.","title":"Create the JSON view"},{"location":"posts/django-auth-server-for-shiny/#proxying-shiny-requests-to-the-shiny-app","text":"This is time for NginX to come on stage. We need to create our new, project-specific configuration file in /etc/nginx/sites-available/djangoshiny . We will tell NginX to proxy every request with an URL like shiny/* to our Shiny app. All other requests will be proxied to the Django app. # declare your Django app upstream djangoapp_server { server localhost : 8000 ; } # declare your Shiny app upstream shinyapp_server { server localhost : 8100 ; } # required for WebSockets map $http_upgrade $connection_upgrade { default upgrade ; '' close ; } server { listen 80 ; server_name localhost ; client_max_body_size 100M ; # normal requests go to Django location / { proxy_pass http://djangoapp_server ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_set_header Host $host ; proxy_redirect off ; if (!-f $request_filename ) { proxy_pass http://djangoapp_server ; break ; } } # \"shiny\" requests go to Shiny location ~ /shiny/.+ { rewrite ^/shiny/(.*) $ / $1 break ; proxy_pass http://shinyapp_server ; proxy_redirect http://shinyapp_server/ $scheme://$host/shiny/ ; # required for WebSockets proxy_http_version 1 .1 ; proxy_set_header Upgrade $http_upgrade ; proxy_set_header Connection $connection_upgrade ; proxy_read_timeout 20d ; proxy_buffering off ; } } Once you're ready, enable this configuration by linking it like this: sudo ln -s /etc/nginx/sites-available/djangoshiny /etc/nginx/sites-enabled/djangoshiny Now reload NginX, and that's it! If you launch both Django on port 8000, and Shiny on port 8100, you should be able to connect to http://localhost/shiny and enjoy your Django-wrapped Shiny application functioning properly! There is still one thing we need to add: authentication and authorization. I won't show how to create sign-in / sign-up / sign-out views: you can find all the documentation to enable Django authentication system online. What I will show you is how to make Django act as an authorization server for Shiny.","title":"Proxying Shiny requests to the Shiny app"},{"location":"posts/django-auth-server-for-shiny/#adding-an-authentication-step-for-every-shiny-request","text":"We want all requests proxied to Shiny to be authorized by Django. For this we are gonna use the auth-request module. This module is not compiled in NginX by default on distributions like Ubuntu / Debian. If you want to recompile NginX with auth-request enabled, check how I do it in this Dockerfile . An easier solution is to use the Docker setup from the same repository, with the official NginX image which already supports auth-request. So, once your NginX is ready, add this authorization step in the configuration file: location ~ /shiny/.+ { # we tell nginx to call that location for each request auth_request /auth ; rewrite ^/shiny/(.*) $ / $1 break ; proxy_pass http://shinyapp_server ; proxy_redirect http://shinyapp_server/ $scheme://$host/shiny/ ; # this part is needed for WebSockets to work proxy_http_version 1 .1 ; proxy_set_header Upgrade $http_upgrade ; proxy_set_header Connection $connection_upgrade ; proxy_read_timeout 20d ; proxy_buffering off ; } # the /auth location will send a subrequest to django, URL shiny_auth/ location = /auth { internal ; proxy_pass http://localhost:8000/shiny_auth/ ; proxy_pass_request_body off ; proxy_set_header Content-Length \"\" ; proxy_set_header X-Original-URI $request_uri ; } Of course, we also need to write the view called by the shiny_auth/ URL. It's a very simple one. First add the URL in urls.py : from django.contrib import admin from django.urls import path from . import views urlpatterns = [ path ( 'admin/' , admin . site . urls ), path ( 'shiny/' , views . shiny , name = 'shiny' ), path ( 'shiny_contents/' , views . shiny_contents , name = 'shiny_contents' ), path ( 'shiny_auth/' , views . shiny_auth ), ] And then the view in views.py : from django.http import HttpResponse def shiny_auth ( request ): if request . user . is_authenticated : return HttpResponse ( status = 200 ) return HttpResponse ( status = 403 ) Et voil\u00e0! If the user is authenticated, Django will say \"OK\". If the user is not authenticated, Django will say \"No\". But you could implement any logic you need instead of just checking if the user is authenticated or not. You could have several Shiny apps served on different URLs, powered by Django (in wrapping pages, as we saw earlier), and grant access to users with a permission system, etc..","title":"Adding an authentication step for every Shiny request"},{"location":"posts/django-auth-server-for-shiny/#try-it-with-a-dockerized-project","text":"Clone the repository and build the images with the following commands: git clone https://github.com/Pawamoy/docker-nginx-auth-request-django-shiny-example docker-django-shiny cd docker-django-shiny sudo make all This first build should take quite some time. It will build Shiny and Django images, get static assets, create the database, create a super user, and then run the application. You will then be able to go to http://localhost:8000 to see it in action. To print the available make commands, simply run make .","title":"Try it with a Dockerized project"},{"location":"posts/django-dashboard-with-suit-and-highcharts/","text":"One day my boss said \"I want to see some statistical data about the users\". I immediately thought about adding a custom page in the admin panel instead of creating a new view with admin restriction. I was already using django-suit which is a great theme (and more) for the Django admin panel, so I searched for a way to add a custom view within Suit. Looking at the issue from brunocascio on the repo, I found the comment written by rouxxx and linking to this post about django dashboard . I had then all I needed to start a proof of concept. Contents Step 1: set up the files Step 2: activate your brand new dashboard Step 3: Highcharts! More customization The app I wrote using this Comments Step 1: set up the files \u00a4 First, create a new app within your Django project: ./manage.py startapp dashboard Then create the additional sites.py module in the new dashboard app. # -*- coding: utf-8 -*- from __future__ import unicode_literals from django.contrib.admin.sites import AdminSite from django.conf.urls import url from dashboard.views import DashboardMainView class AdminMixin ( object ): \"\"\"Mixin for AdminSite to allow custom dashboard views.\"\"\" def get_urls ( self ): \"\"\"Add dashboard view to admin urlconf.\"\"\" urls = super ( AdminMixin , self ) . get_urls () del urls [ 0 ] custom_url = [ url ( r '^$' , self . admin_view ( DashboardMainView . as_view ()), name = \"index\" ) ] return custom_url + urls class DashboardSite ( AdminMixin , AdminSite ): \"\"\" A Django AdminSite with the AdminMixin to allow registering custom dashboard view. \"\"\" pass Next, update the contents of views.py : # -*- coding: utf-8 -*- from __future__ import unicode_literals from django.views.generic import TemplateView class DashboardMainView ( TemplateView ): template_name = 'dashboard/main.html' def get ( self , request , * args , ** kwargs ): context = self . get_context_data ( ** kwargs ) return self . render_to_response ( context = context ) And finally, create the template main.html (thanks brunocascio): {% extends \"admin/base_site.html\" %} {% load i 18 n admin_static %} {% block title %} My title {% endblock %} {% block breadcrumbs %}{% endblock %} {# {% block sidebar %}{% endblock %} #} {# {% block header_time %}{% endblock %} #} {% block content %} My content {% endblock %} Step 2: activate your brand new dashboard \u00a4 Add your dashboard app in your settings.INSTALLED_APPS , but also replace django.contrib.admin by django.contrib.admin.apps.SimpleAdminConfig (thanks rouxxx): INSTALLED_APPS = ( 'suit' , # Beautiful admin interface # Replace 'django.contrib.admin' for dashboard # 'django.contrib.admin', 'django.contrib.admin.apps.SimpleAdminConfig' , # ... 'dashboard' , # ... ) In your main urls.py , you can now use your custom AdminSite: from dashboard.sites import DashboardSite admin . site = DashboardSite () admin . autodiscover () EDIT: with Django 1.9.5, you will need one more line: (See Gustavo's comment at the bottom.) admin . site = DashboardSite () admin . sites . site = admin . site admin . autodiscover () Your urlpatterns should not be subject to change: urlpatterns = [ url ( r '^admin/' , include ( admin . site . urls )), #... ] That's it, go check your new empty dashboard! Step 3: Highcharts! \u00a4 I was already familiar with Highcharts since I use it in my project, so it made sense to use it in the admin dashboard too. Somewhere in your project, you will have functions that compute or retrieve data from your database(s) or else. Add these data in the context dictionary of the DashboardMainView in dashboard/views.py . Example: from core.data import get_country_name_by_code from statistics.sitewide import nb_patients_per_country class DashboardMainView ( TemplateView ): template_name = 'dashboard/main.html' def get ( self , request , * args , ** kwargs ): context = self . get_context_data ( ** kwargs ) context [ 'nb_patients_per_country' ] = sorted ( [{ 'country' : get_country_name_by_code ( k ), 'patients' : v } for k , v in nb_patients_per_country () . items () if v > 0 ], key = lambda x : x [ 'patients' ], reverse = True ) # I filtered the 0 values out, and used get_country_name_by_code to # have the countries names translated! return self . render_to_response ( context = context ) Then in your template, add links to Highchart, and use the previous data to create, for example, a pie chart: {% block content %} < script src = \"http://code.highcharts.com/highcharts.js\" ></ script > < script src = \"http://code.highcharts.com/highcharts-more.js\" ></ script > < script src = \"http://code.highcharts.com/modules/exporting.js\" ></ script > < div id = \"highchart-0\" ></ div > < script type = 'text/javascript' > var dataset ; dataset = { \"chart\" : { \"type\" : \"pie\" , \"plotBackgroundColor\" : null , \"plotBorderWidth\" : null , \"plotShadow\" : false }, \"title\" : { \"text\" : \"{% trans \" Number of patients per country \" %}\" }, \"series\" : [{ \"name\" : \"Countries\" , \"colorByPoint\" : true , \"data\" : [ { % for tuple in nb_patients_per_country % } { \"name\" : \"{{ tuple.country }}\" , \"y\" : {{ tuple . patients }} }, { % endfor % } ] }], \"tooltip\" : { \"formatter\" : function () { return this . y + '/' + this . total + ' (' + Highcharts . numberFormat ( this . percentage , 1 ) + '%)' ; } }, \"plotOptions\" : { \"pie\" : { \"showInLegend\" : true , \"allowPointSelect\" : true , \"cursor\" : \"pointer\" , \"dataLabels\" : { \"enabled\" : true , \"format\" : \"<b>{point.name}</b>: {point.percentage:.1f} %\" \"style\" : { \"color\" : function () { return ( Highcharts . theme && Highcharts . theme . contrastTextColor ) || 'black' } } } } } }; $ ( '#highchart-0' ). highcharts ( dataset ); </ script > {% endblock %} Here is the result: For other types of chart, go check the Highcharts demos and their excellent API documentation ! More customization \u00a4 Customizable contents \u00a4 Work with columns Add more pages to your dashboard Columns \u00a4 If like me you would like to use columns in the dashboard, you will be disappointed since we cannot use Bootstrap's column classes ( col-md and stuff) for I don't know what reason. Instead, Suit provides the suit-column class which lets you stack multiple columns on one row. But you can't specify the width and they won't have the same size. Here is a CSS starting point to poorly imitate Bootstrap's columns: . suit-row-1-col . suit-column { width : 100 % ; } . suit-row-2-col . suit-column { width : 50 % ; } . suit-row-3-col . suit-column { width : 33.33 % ; } . suit-row-4-col . suit-column { width : 25 % ; } . suit-row-5-col . suit-column { width : 20 % ; } . suit-row-6-col . suit-column { width : 16.66 % ; } . suit-row-7-col . suit-column { width : 14.28 % ; } . suit-row-8-col . suit-column { width : 12.5 % ; } . suit-row-9-col . suit-column { width : 11.11 % ; } . suit-row-10-col . suit-column { width : 10 % ; } . suit-row-11-col . suit-column { width : 9.09 % ; } . suit-row-12-col . suit-column { width : 8.333 % ; } ...to use like this: < div class = \"suit-row suit-row-3-col\" > < div class = \"suit-column\" > < p > 1 </ p > </ div > < div class = \"suit-column\" > < p > 2 </ p > </ div > < div class = \"suit-column\" > < p > 3 </ p > </ div > </ div > ...and which results like this: But this tweak is very limited because you can't have columns with different sizes on the same row... More dashboard pages \u00a4 Just add your additional URLs in dashboard.sites.AdminMixin (they should maybe be added at the end of the original URLs: return custom_url + urls + additional_urls ), create the views and the templates and it's done. Django Suit Dashboard \u00a4 I wrote a Django app to do this: django-suit-dashboard . Go take a look! (and please keep in mind it's just a beta and it could/will be improved).","title":"Django admin dashboard with Suit and Highcharts"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#step-1-set-up-the-files","text":"First, create a new app within your Django project: ./manage.py startapp dashboard Then create the additional sites.py module in the new dashboard app. # -*- coding: utf-8 -*- from __future__ import unicode_literals from django.contrib.admin.sites import AdminSite from django.conf.urls import url from dashboard.views import DashboardMainView class AdminMixin ( object ): \"\"\"Mixin for AdminSite to allow custom dashboard views.\"\"\" def get_urls ( self ): \"\"\"Add dashboard view to admin urlconf.\"\"\" urls = super ( AdminMixin , self ) . get_urls () del urls [ 0 ] custom_url = [ url ( r '^$' , self . admin_view ( DashboardMainView . as_view ()), name = \"index\" ) ] return custom_url + urls class DashboardSite ( AdminMixin , AdminSite ): \"\"\" A Django AdminSite with the AdminMixin to allow registering custom dashboard view. \"\"\" pass Next, update the contents of views.py : # -*- coding: utf-8 -*- from __future__ import unicode_literals from django.views.generic import TemplateView class DashboardMainView ( TemplateView ): template_name = 'dashboard/main.html' def get ( self , request , * args , ** kwargs ): context = self . get_context_data ( ** kwargs ) return self . render_to_response ( context = context ) And finally, create the template main.html (thanks brunocascio): {% extends \"admin/base_site.html\" %} {% load i 18 n admin_static %} {% block title %} My title {% endblock %} {% block breadcrumbs %}{% endblock %} {# {% block sidebar %}{% endblock %} #} {# {% block header_time %}{% endblock %} #} {% block content %} My content {% endblock %}","title":"Step 1: set up the files"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#step-2-activate-your-brand-new-dashboard","text":"Add your dashboard app in your settings.INSTALLED_APPS , but also replace django.contrib.admin by django.contrib.admin.apps.SimpleAdminConfig (thanks rouxxx): INSTALLED_APPS = ( 'suit' , # Beautiful admin interface # Replace 'django.contrib.admin' for dashboard # 'django.contrib.admin', 'django.contrib.admin.apps.SimpleAdminConfig' , # ... 'dashboard' , # ... ) In your main urls.py , you can now use your custom AdminSite: from dashboard.sites import DashboardSite admin . site = DashboardSite () admin . autodiscover () EDIT: with Django 1.9.5, you will need one more line: (See Gustavo's comment at the bottom.) admin . site = DashboardSite () admin . sites . site = admin . site admin . autodiscover () Your urlpatterns should not be subject to change: urlpatterns = [ url ( r '^admin/' , include ( admin . site . urls )), #... ] That's it, go check your new empty dashboard!","title":"Step 2: activate your brand new dashboard"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#step-3-highcharts","text":"I was already familiar with Highcharts since I use it in my project, so it made sense to use it in the admin dashboard too. Somewhere in your project, you will have functions that compute or retrieve data from your database(s) or else. Add these data in the context dictionary of the DashboardMainView in dashboard/views.py . Example: from core.data import get_country_name_by_code from statistics.sitewide import nb_patients_per_country class DashboardMainView ( TemplateView ): template_name = 'dashboard/main.html' def get ( self , request , * args , ** kwargs ): context = self . get_context_data ( ** kwargs ) context [ 'nb_patients_per_country' ] = sorted ( [{ 'country' : get_country_name_by_code ( k ), 'patients' : v } for k , v in nb_patients_per_country () . items () if v > 0 ], key = lambda x : x [ 'patients' ], reverse = True ) # I filtered the 0 values out, and used get_country_name_by_code to # have the countries names translated! return self . render_to_response ( context = context ) Then in your template, add links to Highchart, and use the previous data to create, for example, a pie chart: {% block content %} < script src = \"http://code.highcharts.com/highcharts.js\" ></ script > < script src = \"http://code.highcharts.com/highcharts-more.js\" ></ script > < script src = \"http://code.highcharts.com/modules/exporting.js\" ></ script > < div id = \"highchart-0\" ></ div > < script type = 'text/javascript' > var dataset ; dataset = { \"chart\" : { \"type\" : \"pie\" , \"plotBackgroundColor\" : null , \"plotBorderWidth\" : null , \"plotShadow\" : false }, \"title\" : { \"text\" : \"{% trans \" Number of patients per country \" %}\" }, \"series\" : [{ \"name\" : \"Countries\" , \"colorByPoint\" : true , \"data\" : [ { % for tuple in nb_patients_per_country % } { \"name\" : \"{{ tuple.country }}\" , \"y\" : {{ tuple . patients }} }, { % endfor % } ] }], \"tooltip\" : { \"formatter\" : function () { return this . y + '/' + this . total + ' (' + Highcharts . numberFormat ( this . percentage , 1 ) + '%)' ; } }, \"plotOptions\" : { \"pie\" : { \"showInLegend\" : true , \"allowPointSelect\" : true , \"cursor\" : \"pointer\" , \"dataLabels\" : { \"enabled\" : true , \"format\" : \"<b>{point.name}</b>: {point.percentage:.1f} %\" \"style\" : { \"color\" : function () { return ( Highcharts . theme && Highcharts . theme . contrastTextColor ) || 'black' } } } } } }; $ ( '#highchart-0' ). highcharts ( dataset ); </ script > {% endblock %} Here is the result: For other types of chart, go check the Highcharts demos and their excellent API documentation !","title":"Step 3: Highcharts!"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#more-customization","text":"","title":"More customization"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#customizable-contents","text":"Work with columns Add more pages to your dashboard","title":"Customizable contents"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#columns","text":"If like me you would like to use columns in the dashboard, you will be disappointed since we cannot use Bootstrap's column classes ( col-md and stuff) for I don't know what reason. Instead, Suit provides the suit-column class which lets you stack multiple columns on one row. But you can't specify the width and they won't have the same size. Here is a CSS starting point to poorly imitate Bootstrap's columns: . suit-row-1-col . suit-column { width : 100 % ; } . suit-row-2-col . suit-column { width : 50 % ; } . suit-row-3-col . suit-column { width : 33.33 % ; } . suit-row-4-col . suit-column { width : 25 % ; } . suit-row-5-col . suit-column { width : 20 % ; } . suit-row-6-col . suit-column { width : 16.66 % ; } . suit-row-7-col . suit-column { width : 14.28 % ; } . suit-row-8-col . suit-column { width : 12.5 % ; } . suit-row-9-col . suit-column { width : 11.11 % ; } . suit-row-10-col . suit-column { width : 10 % ; } . suit-row-11-col . suit-column { width : 9.09 % ; } . suit-row-12-col . suit-column { width : 8.333 % ; } ...to use like this: < div class = \"suit-row suit-row-3-col\" > < div class = \"suit-column\" > < p > 1 </ p > </ div > < div class = \"suit-column\" > < p > 2 </ p > </ div > < div class = \"suit-column\" > < p > 3 </ p > </ div > </ div > ...and which results like this: But this tweak is very limited because you can't have columns with different sizes on the same row...","title":"Columns"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#more-dashboard-pages","text":"Just add your additional URLs in dashboard.sites.AdminMixin (they should maybe be added at the end of the original URLs: return custom_url + urls + additional_urls ), create the views and the templates and it's done.","title":"More dashboard pages"},{"location":"posts/django-dashboard-with-suit-and-highcharts/#django-suit-dashboard","text":"I wrote a Django app to do this: django-suit-dashboard . Go take a look! (and please keep in mind it's just a beta and it could/will be improved).","title":"Django Suit Dashboard"},{"location":"posts/docker-compose-django-postgres-nginx/","text":"This post explains how to setup your Docker configuration for a web application based on the Django framework. I got a lot of inspiration from other tutorials and Docker examples: you can check these resources with the links at the bottom of the post. You can also directly check the repository that reflects this tutorial. In this particular example, we will use Gunicorn, but it should be easy enough to replace it with an alternative Python WSGI HTTP server such as uwsgi. We will also make use of pipenv, with related Pipfile and Pipfile.lock, instead of plain pip and requirements.txt files. Here is the plan: Overview: to get a better understanding of the whole thing Dockerfile: a simple Django application served by Gunicorn Pipenv: spice things up with Pipfile and Pipfile.lock Compose: add a container for NginX Compose: add containers for one or more Postgres databases Static files: collecting, storing and serving Resources Go to comments Overview: to get a better understanding of the whole thing \u00a4 So, let's start with some drawings in order to get a better idea of what we want to accomplish, and how everything will fit together. In this first, very simple image, you can see that we want three containers: one for NginX, one for Django + Gunicorn (they always go together), and one for our database. The NginX container communicate with the Django+Gunicorn one, which itself connects to the Postgres container. Pretty straight-forward, right? In our configuration, it means we will declare three containers, or three services if we talk in terms of Docker Compose. Except that we need bridges between the containers, in order for them to communicate. Let's add these bridges: In docker-compose.yml , we will declare these bridges thanks to the networks directive, and connect them to the right containers. Of course, you may want or need several databases for your project. So here is an updated image with two database containers. It's simply a matter of adding a new brige: Once you know how to do it for two databases, it's very easy to add more. Now, this is enough for local development. But each time you restart your containers or services, the data in the Postgres databases will be lost. In production, we need these data to be persistent. If we keep the data in production, let's keep them in local environment as well. To do this, we will use volumes, a feature of Docker: Alright, that is enough for the overview, let's get our hands dirty! Dockerfile: a simple Django application served by Gunicorn \u00a4 If you don't already have a simple Django project available for testing, I invite you to create one with django-admin startproject hello . Here is the directory/file tree you should have in order to follow this tutorial: . # Your current directory, created for this tutorial \u2514\u2500\u2500 hello # The Django project \u251c\u2500\u2500 hello # The main Django app of your project \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 settings.py \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2514\u2500\u2500 wsgi.py \u2514\u2500\u2500 manage.py Now that you have a working Django project, you can run it by going into the hello directory and type ./manage.py runserver . Go to http://localhost:8000 to see the result. Instead of running it with the Django runserver management command, let's try with Gunicorn. First, install it with pip install gunicorn , be it in a virtualenv or system-wide with sudo pip install gunicorn . It's as easy as running gunicorn --bind :8000 hello.wsgi:application from inside the Django project. If you are one directory above, use gunicorn --chdir hello --bind :8000 hello.wsgi:application . We have all we need to write our Dockerfile: # start from an official image FROM python:3.6 # arbitrary location choice: you can change the directory RUN mkdir -p /opt/services/djangoapp/src WORKDIR /opt/services/djangoapp/src # install our two dependencies RUN pip install gunicorn django # copy our project code COPY . /opt/services/djangoapp/src # expose the port 8000 EXPOSE 8000 # define the default command to run when starting the container CMD [ \"gunicorn\" , \"--chdir\" , \"hello\" , \"--bind\" , \":8000\" , \"hello.wsgi:application\" ] The Dockerfile must be placed at the root of your test directory. As a reminder: . # Your current directory, created for this tutorial \u251c\u2500\u2500 hello # The Django project \u2502 \u251c\u2500\u2500 hello # The main Django app of your project \u2502 \u2514\u2500\u2500 manage.py \u2514\u2500\u2500 Dockerfile # Your Dockerfile We are now able to build our container with docker build . -t hello , and to start it with docker run -p 8000:8000 hello . The -p 8000:8000 option says to bind the port 8000 of the host to the port 8000 of the container, allowing you to go to http://localhost:8000 and see your application running as if you were inside of the container. Pipenv: spice things up with Pipfile and Pipfile.lock \u00a4 This step is completely optional. If you prefer to use plain pip and requirements files, you can skip this section. First install pipenv with pip install pipenv , or system-wide with sudo pip install pipenv . Since we only need Django and Gunicorn, our Pipfile will be very simple: [[source]] url = \"https://pypi.python.org/simple\" verify_ssl = true name = \"pypi\" [packages] Django = \"*\" gunicorn = \"*\" [requires] # our Dockerfile is based on Python 3.6 python_version = \"3.6\" Just like the Dockerfile, Pipfile must be placed at the root of the project. . \u251c\u2500\u2500 hello \u2502 \u251c\u2500\u2500 hello \u2502 \u2514\u2500\u2500 manage.py \u251c\u2500\u2500 Dockerfile \u2514\u2500\u2500 Pipfile Simply run pipenv lock to create Pipfile.lock from Pipfile. Now we need to update our Dockerfile to use pipenv: # start from an official image FROM python:3.6 # arbitrary location choice: you can change the directory RUN mkdir -p /opt/services/djangoapp/src WORKDIR /opt/services/djangoapp/src # install our dependencies # we use --system flag because we don't need an extra virtualenv COPY Pipfile Pipfile.lock /opt/services/djangoapp/src/ RUN pip install pipenv && pipenv install --system # copy our project code COPY . /opt/services/djangoapp/src # expose the port 8000 EXPOSE 8000 # define the default command to run when starting the container CMD [ \"gunicorn\" , \"--chdir\" , \"hello\" , \"--bind\" , \":8000\" , \"hello.wsgi:application\" ] You can rebuild the image with docker build . -t hello and try to run it again to see if everything works correctly. Compose: add a container for NginX \u00a4 Since we will then have two containers, one for Django + Gunicorn, and one for NginX, it's time to start our composition with Docker Compose and docker-compose.yml . Create your docker-compose.yml file at the root of the project, like following: . \u251c\u2500\u2500 hello \u2502 \u251c\u2500\u2500 hello \u2502 \u2514\u2500\u2500 manage.py \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 Dockerfile \u2514\u2500\u2500 Pipfile We are gonna use the version 3 of the configuration syntax. First, we add the Django+Gunicorn service: version : '3' services : djangoapp : build : . volumes : - .:/opt/services/djangoapp/src ports : - 8000:8000 We simply tell Docker Compose that the djangoapp service must use an image that is built from the current directory, therefore looking for our Dockerfile. The volumes directive tells to bind the current directory of the host to the /opt/services/djangoapp/src directory of the container. The changes in our current directory will be reflected in real-time in the container directory. And reciprocally, changes that occur in the container directory will occur in our current directory as well. Build and run the service with docker-compose up . The name of the image will be automatically chosen by Docker Compose (it will be the name of the current directory with _djangoapp appended). Ok, let's add our NginX service now: version : '3' services : djangoapp : build : . volumes : - .:/opt/services/djangoapp/src nginx : image : nginx:1.13 ports : - 8000:80 volumes : - ./config/nginx/conf.d:/etc/nginx/conf.d depends_on : # <-- wait for djangoapp to be \"ready\" before starting this service - djangoapp Note that we removed the ports directive from our djangoapp service. Indeed we will not communicate directly with Gunicorn anymore, but with NginX. We still want to access our app at http://localhost:8000 , and we want NginX to listen to the port 80 in the container, so we use ports: - 8000:80 . Note: in a production environment, we would use 80:80 instead. We also bind a local directory to the /etc/nginx/conf.d container directory. Let's create it and see what's inside: mkdir -p config/nginx/conf.d touch config/nginx/conf.d/local.conf You should now have the following files and directories: . \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 nginx \u2502 \u2514\u2500\u2500 conf.d \u2502 \u2514\u2500\u2500 local.conf \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 hello \u2502 \u251c\u2500\u2500 hello \u2502 \u2514\u2500\u2500 manage.py \u251c\u2500\u2500 Pipfile \u2514\u2500\u2500 Pipfile.lock The config/nginx/conf.d/local.conf file contains our NginX configuration: # first we declare our upstream server, which is our Gunicorn application upstream hello_server { # docker will automatically resolve this to the correct address # because we use the same name as the service: \"djangoapp\" server djangoapp : 8000 ; } # now we declare our main server server { listen 80 ; server_name localhost ; location / { # everything is passed to Gunicorn proxy_pass http://hello_server ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_set_header Host $host ; proxy_redirect off ; } } But before we try this out, remember that we need a bridge to make our services able to communicate? Update your docker-compose.yml as follow: version : '3' services : djangoapp : build : . volumes : - .:/opt/services/djangoapp/src networks : # <-- here - nginx_network nginx : image : nginx:1.13 ports : - 8000:80 volumes : - ./config/nginx/conf.d:/etc/nginx/conf.d depends_on : - djangoapp networks : # <-- here - nginx_network networks : # <-- and here nginx_network : driver : bridge Run docker-compose up and see if you can still see the Django default page at http://localhost:8000 . Compose: add containers for one or more Postgres databases \u00a4 We now want to use Postgres instead of the starting default SQLite database. We will need to update several things: our Pipfile, because we need the psycopg2 Python package, the Postgres driver; our Django project settings; and our docker-compose.yml file. Pipfile becomes: [[source]] url = \"https://pypi.python.org/simple\" verify_ssl = true name = \"pypi\" [packages] Django = \"*\" gunicorn = \"*\" \"psycopg2\" = \"*\" [requires] # our Dockerfile is based on Python 3.6 python_version = \"3.6\" Don't forget to run pipenv lock to update your lock file, and rebuild your Docker image with docker-compose build . In the Django project settings, update the DATABASE setting from: DATABASES = { 'default' : { 'ENGINE' : 'django.db.backends.sqlite3' , 'NAME' : os . path . join ( BASE_DIR , 'db.sqlite3' ), } } ...to: DATABASES = { 'default' : { 'ENGINE' : 'django.db.backends.postgresql_psycopg2' , 'NAME' : 'database1' , 'USER' : 'database1_role' , 'PASSWORD' : 'database1_password' , 'HOST' : 'database1' , # <-- IMPORTANT: same name as docker-compose service! 'PORT' : '5432' , } } As you can see, we used database1 everywhere, for the name, user, password and host. In fact, we can change these values to whatever suits us. But we must ensure the database container will use the same values! To do that, we will copy these values in a configuration file destined to be read by our database container. Create a db directory in the config one, and add the database1_env file: mkdir config/db touch config/db/database1_env The contents of config/db/database1_env must then be: POSTGRES_USER=database1_role POSTGRES_PASSWORD=database1_password POSTGRES_DB=database1 These variable are used by the Postgres Docker image, for more information please check out the documentation on docs.docker.com or hub.docker.com . It means that, when started, the Postgres container will create a database called database1 , assigned to the role database1_role with password database1_password . If you change these values, remember to also change them in the DATABASES setting. We are now ready to add our service in docker-compose.yml . The added service must have the same name than what is declared in the DATABASES setting: version : '3' services : djangoapp : build : . volumes : - .:/opt/services/djangoapp/src networks : - nginx_network - database1_network # <-- connect to the bridge depends_on : # <-- wait for db to be \"ready\" before starting the app - database1 nginx : image : nginx:1.13 ports : - 8000:80 volumes : - ./config/nginx/conf.d:/etc/nginx/conf.d depends_on : - djangoapp networks : - nginx_network database1 : # <-- IMPORTANT: same name as in DATABASES setting, otherwise Django won't find the database! image : postgres:10 env_file : # <-- we use the previously defined values - config/db/database1_env networks : # <-- connect to the bridge - database1_network volumes : - database1_volume:/var/lib/postgresql/data networks : nginx_network : driver : bridge database1_network : # <-- add the bridge driver : bridge volumes : database1_volume : You should be able to understand everything here. However, we added two new things: the database1: volumes: directive, and the root volumes: directive. You need to declare your volumes in the root volumes: directive if you want them to be kept persistently. Then, you can bind a volume to a directory in the container. Here, we bind our declared database1_volume to the database1 container's /var/lib/postgresql/data directory. Everything added to this directory will be persistently stored in the volume called database1_volume . So each subsequent run of the container will have access to the previous data! It means you can stop and restart your service without losing the data. OK, let's try it. As we are using Django, we need to \"migrate\" the database first. To do this, we will simply use Docker Compose to start our djangoapp service and run the migration command inside it: docker-compose build # to make sure everything is up-to-date docker-compose run --rm djangoapp /bin/bash -c \"cd hello; ./manage.py migrate\" From now on, it should be really easy to add other databases: just add other database services ( database2 ) with their networks volumes (remember to connect the networks and bind the volumes), update your DATABASES setting in the Django project, and create the environment file for each database in config/db . Static files: collecting, storing and serving \u00a4 UPDATE : a lot of us (including myself) seem to have trouble updating the static files in the designated Docker volume. In retrospect, collecting the static files in the Dockerfile might not be a good idea, because of the caching mechanisms of Docker. A simple and effective solution, mentioned by Isaac T Chikutukutu in the comments, is to remove this command from the Dockerfile and manually run it when needed: docker-compose run djangoapp hello/manage.py collectstatic --no-input I edited the rest of the post accordingly. (End of update) Let's not forget about the static files! In order for NginX to serve them, we will update the config/nginx/conf.d/local.conf file, as well as our docker-compose.yml file. Static files will be stored in volumes. We also need to set the STATIC_ROOT and MEDIA_ROOT variables in the Django project settings. NginX configuration: upstream hello_server { server djangoapp : 8000 ; } server { listen 80 ; server_name localhost ; location / { proxy_pass http://hello_server ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_set_header Host $host ; proxy_redirect off ; } location /static/ { alias /opt/services/djangoapp/static/ ; } location /media/ { alias /opt/services/djangoapp/media/ ; } } Django project settings: # as declared in NginX conf, it must match /opt/services/djangoapp/static/ STATIC_ROOT = os . path . join ( os . path . dirname ( os . path . dirname ( BASE_DIR )), 'static' ) # do the same for media files, it must match /opt/services/djangoapp/media/ MEDIA_ROOT = os . path . join ( os . path . dirname ( os . path . dirname ( BASE_DIR )), 'media' ) Volumes in docker-compose.yml : version : '3' services : djangoapp : build : . volumes : - .:/opt/services/djangoapp/src - static_volume:/opt/services/djangoapp/static # <-- bind the static volume - media_volume:/opt/services/djangoapp/media # <-- bind the media volume networks : - nginx_network - database1_network depends_on : - database1 nginx : image : nginx:1.13 ports : - 8000:80 volumes : - ./config/nginx/conf.d:/etc/nginx/conf.d - static_volume:/opt/services/djangoapp/static # <-- bind the static volume - media_volume:/opt/services/djangoapp/media # <-- bind the media volume depends_on : - djangoapp networks : - nginx_network database1 : image : postgres:10 env_file : - config/db/database1_env networks : - database1_network volumes : - database1_volume:/var/lib/postgresql/data networks : nginx_network : driver : bridge database1_network : driver : bridge volumes : database1_volume : static_volume : # <-- declare the static volume media_volume : # <-- declare the media volume And finally, collect the static files each time you need to update them, by running: docker-compose run djangoapp hello/manage.py collectstatic --no-input Resources \u00a4 Here are the resources I used to write this tutorial: Nginx+Flask+Postgres multi-container setup with Docker Compose The repository Docker how to Django + uwsgi/gunicorn + nginx? Django tutorial using Docker, Nginx, Gunicorn and PostgreSQL. Django Development With Docker Compose and Machine Deploy Django, Gunicorn, NGINX, Postgresql using Docker Docker, how to expose a socket over a port for a Django Application And here is the repository that reflects this tutorial (with a few more things). Don't hesitate to share other interesting resources in the comment section!","title":"Docker Compose with NginX, Django, Gunicorn and multiple Postgres databases"},{"location":"posts/docker-compose-django-postgres-nginx/#overview-to-get-a-better-understanding-of-the-whole-thing","text":"So, let's start with some drawings in order to get a better idea of what we want to accomplish, and how everything will fit together. In this first, very simple image, you can see that we want three containers: one for NginX, one for Django + Gunicorn (they always go together), and one for our database. The NginX container communicate with the Django+Gunicorn one, which itself connects to the Postgres container. Pretty straight-forward, right? In our configuration, it means we will declare three containers, or three services if we talk in terms of Docker Compose. Except that we need bridges between the containers, in order for them to communicate. Let's add these bridges: In docker-compose.yml , we will declare these bridges thanks to the networks directive, and connect them to the right containers. Of course, you may want or need several databases for your project. So here is an updated image with two database containers. It's simply a matter of adding a new brige: Once you know how to do it for two databases, it's very easy to add more. Now, this is enough for local development. But each time you restart your containers or services, the data in the Postgres databases will be lost. In production, we need these data to be persistent. If we keep the data in production, let's keep them in local environment as well. To do this, we will use volumes, a feature of Docker: Alright, that is enough for the overview, let's get our hands dirty!","title":"Overview: to get a better understanding of the whole thing"},{"location":"posts/docker-compose-django-postgres-nginx/#dockerfile-a-simple-django-application-served-by-gunicorn","text":"If you don't already have a simple Django project available for testing, I invite you to create one with django-admin startproject hello . Here is the directory/file tree you should have in order to follow this tutorial: . # Your current directory, created for this tutorial \u2514\u2500\u2500 hello # The Django project \u251c\u2500\u2500 hello # The main Django app of your project \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 settings.py \u2502 \u251c\u2500\u2500 urls.py \u2502 \u2514\u2500\u2500 wsgi.py \u2514\u2500\u2500 manage.py Now that you have a working Django project, you can run it by going into the hello directory and type ./manage.py runserver . Go to http://localhost:8000 to see the result. Instead of running it with the Django runserver management command, let's try with Gunicorn. First, install it with pip install gunicorn , be it in a virtualenv or system-wide with sudo pip install gunicorn . It's as easy as running gunicorn --bind :8000 hello.wsgi:application from inside the Django project. If you are one directory above, use gunicorn --chdir hello --bind :8000 hello.wsgi:application . We have all we need to write our Dockerfile: # start from an official image FROM python:3.6 # arbitrary location choice: you can change the directory RUN mkdir -p /opt/services/djangoapp/src WORKDIR /opt/services/djangoapp/src # install our two dependencies RUN pip install gunicorn django # copy our project code COPY . /opt/services/djangoapp/src # expose the port 8000 EXPOSE 8000 # define the default command to run when starting the container CMD [ \"gunicorn\" , \"--chdir\" , \"hello\" , \"--bind\" , \":8000\" , \"hello.wsgi:application\" ] The Dockerfile must be placed at the root of your test directory. As a reminder: . # Your current directory, created for this tutorial \u251c\u2500\u2500 hello # The Django project \u2502 \u251c\u2500\u2500 hello # The main Django app of your project \u2502 \u2514\u2500\u2500 manage.py \u2514\u2500\u2500 Dockerfile # Your Dockerfile We are now able to build our container with docker build . -t hello , and to start it with docker run -p 8000:8000 hello . The -p 8000:8000 option says to bind the port 8000 of the host to the port 8000 of the container, allowing you to go to http://localhost:8000 and see your application running as if you were inside of the container.","title":"Dockerfile: a simple Django application served by Gunicorn"},{"location":"posts/docker-compose-django-postgres-nginx/#pipenv-spice-things-up-with-pipfile-and-pipfilelock","text":"This step is completely optional. If you prefer to use plain pip and requirements files, you can skip this section. First install pipenv with pip install pipenv , or system-wide with sudo pip install pipenv . Since we only need Django and Gunicorn, our Pipfile will be very simple: [[source]] url = \"https://pypi.python.org/simple\" verify_ssl = true name = \"pypi\" [packages] Django = \"*\" gunicorn = \"*\" [requires] # our Dockerfile is based on Python 3.6 python_version = \"3.6\" Just like the Dockerfile, Pipfile must be placed at the root of the project. . \u251c\u2500\u2500 hello \u2502 \u251c\u2500\u2500 hello \u2502 \u2514\u2500\u2500 manage.py \u251c\u2500\u2500 Dockerfile \u2514\u2500\u2500 Pipfile Simply run pipenv lock to create Pipfile.lock from Pipfile. Now we need to update our Dockerfile to use pipenv: # start from an official image FROM python:3.6 # arbitrary location choice: you can change the directory RUN mkdir -p /opt/services/djangoapp/src WORKDIR /opt/services/djangoapp/src # install our dependencies # we use --system flag because we don't need an extra virtualenv COPY Pipfile Pipfile.lock /opt/services/djangoapp/src/ RUN pip install pipenv && pipenv install --system # copy our project code COPY . /opt/services/djangoapp/src # expose the port 8000 EXPOSE 8000 # define the default command to run when starting the container CMD [ \"gunicorn\" , \"--chdir\" , \"hello\" , \"--bind\" , \":8000\" , \"hello.wsgi:application\" ] You can rebuild the image with docker build . -t hello and try to run it again to see if everything works correctly.","title":"Pipenv: spice things up with Pipfile and Pipfile.lock"},{"location":"posts/docker-compose-django-postgres-nginx/#compose-add-a-container-for-nginx","text":"Since we will then have two containers, one for Django + Gunicorn, and one for NginX, it's time to start our composition with Docker Compose and docker-compose.yml . Create your docker-compose.yml file at the root of the project, like following: . \u251c\u2500\u2500 hello \u2502 \u251c\u2500\u2500 hello \u2502 \u2514\u2500\u2500 manage.py \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 Dockerfile \u2514\u2500\u2500 Pipfile We are gonna use the version 3 of the configuration syntax. First, we add the Django+Gunicorn service: version : '3' services : djangoapp : build : . volumes : - .:/opt/services/djangoapp/src ports : - 8000:8000 We simply tell Docker Compose that the djangoapp service must use an image that is built from the current directory, therefore looking for our Dockerfile. The volumes directive tells to bind the current directory of the host to the /opt/services/djangoapp/src directory of the container. The changes in our current directory will be reflected in real-time in the container directory. And reciprocally, changes that occur in the container directory will occur in our current directory as well. Build and run the service with docker-compose up . The name of the image will be automatically chosen by Docker Compose (it will be the name of the current directory with _djangoapp appended). Ok, let's add our NginX service now: version : '3' services : djangoapp : build : . volumes : - .:/opt/services/djangoapp/src nginx : image : nginx:1.13 ports : - 8000:80 volumes : - ./config/nginx/conf.d:/etc/nginx/conf.d depends_on : # <-- wait for djangoapp to be \"ready\" before starting this service - djangoapp Note that we removed the ports directive from our djangoapp service. Indeed we will not communicate directly with Gunicorn anymore, but with NginX. We still want to access our app at http://localhost:8000 , and we want NginX to listen to the port 80 in the container, so we use ports: - 8000:80 . Note: in a production environment, we would use 80:80 instead. We also bind a local directory to the /etc/nginx/conf.d container directory. Let's create it and see what's inside: mkdir -p config/nginx/conf.d touch config/nginx/conf.d/local.conf You should now have the following files and directories: . \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 nginx \u2502 \u2514\u2500\u2500 conf.d \u2502 \u2514\u2500\u2500 local.conf \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 hello \u2502 \u251c\u2500\u2500 hello \u2502 \u2514\u2500\u2500 manage.py \u251c\u2500\u2500 Pipfile \u2514\u2500\u2500 Pipfile.lock The config/nginx/conf.d/local.conf file contains our NginX configuration: # first we declare our upstream server, which is our Gunicorn application upstream hello_server { # docker will automatically resolve this to the correct address # because we use the same name as the service: \"djangoapp\" server djangoapp : 8000 ; } # now we declare our main server server { listen 80 ; server_name localhost ; location / { # everything is passed to Gunicorn proxy_pass http://hello_server ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_set_header Host $host ; proxy_redirect off ; } } But before we try this out, remember that we need a bridge to make our services able to communicate? Update your docker-compose.yml as follow: version : '3' services : djangoapp : build : . volumes : - .:/opt/services/djangoapp/src networks : # <-- here - nginx_network nginx : image : nginx:1.13 ports : - 8000:80 volumes : - ./config/nginx/conf.d:/etc/nginx/conf.d depends_on : - djangoapp networks : # <-- here - nginx_network networks : # <-- and here nginx_network : driver : bridge Run docker-compose up and see if you can still see the Django default page at http://localhost:8000 .","title":"Compose: add a container for NginX"},{"location":"posts/docker-compose-django-postgres-nginx/#compose-add-containers-for-one-or-more-postgres-databases","text":"We now want to use Postgres instead of the starting default SQLite database. We will need to update several things: our Pipfile, because we need the psycopg2 Python package, the Postgres driver; our Django project settings; and our docker-compose.yml file. Pipfile becomes: [[source]] url = \"https://pypi.python.org/simple\" verify_ssl = true name = \"pypi\" [packages] Django = \"*\" gunicorn = \"*\" \"psycopg2\" = \"*\" [requires] # our Dockerfile is based on Python 3.6 python_version = \"3.6\" Don't forget to run pipenv lock to update your lock file, and rebuild your Docker image with docker-compose build . In the Django project settings, update the DATABASE setting from: DATABASES = { 'default' : { 'ENGINE' : 'django.db.backends.sqlite3' , 'NAME' : os . path . join ( BASE_DIR , 'db.sqlite3' ), } } ...to: DATABASES = { 'default' : { 'ENGINE' : 'django.db.backends.postgresql_psycopg2' , 'NAME' : 'database1' , 'USER' : 'database1_role' , 'PASSWORD' : 'database1_password' , 'HOST' : 'database1' , # <-- IMPORTANT: same name as docker-compose service! 'PORT' : '5432' , } } As you can see, we used database1 everywhere, for the name, user, password and host. In fact, we can change these values to whatever suits us. But we must ensure the database container will use the same values! To do that, we will copy these values in a configuration file destined to be read by our database container. Create a db directory in the config one, and add the database1_env file: mkdir config/db touch config/db/database1_env The contents of config/db/database1_env must then be: POSTGRES_USER=database1_role POSTGRES_PASSWORD=database1_password POSTGRES_DB=database1 These variable are used by the Postgres Docker image, for more information please check out the documentation on docs.docker.com or hub.docker.com . It means that, when started, the Postgres container will create a database called database1 , assigned to the role database1_role with password database1_password . If you change these values, remember to also change them in the DATABASES setting. We are now ready to add our service in docker-compose.yml . The added service must have the same name than what is declared in the DATABASES setting: version : '3' services : djangoapp : build : . volumes : - .:/opt/services/djangoapp/src networks : - nginx_network - database1_network # <-- connect to the bridge depends_on : # <-- wait for db to be \"ready\" before starting the app - database1 nginx : image : nginx:1.13 ports : - 8000:80 volumes : - ./config/nginx/conf.d:/etc/nginx/conf.d depends_on : - djangoapp networks : - nginx_network database1 : # <-- IMPORTANT: same name as in DATABASES setting, otherwise Django won't find the database! image : postgres:10 env_file : # <-- we use the previously defined values - config/db/database1_env networks : # <-- connect to the bridge - database1_network volumes : - database1_volume:/var/lib/postgresql/data networks : nginx_network : driver : bridge database1_network : # <-- add the bridge driver : bridge volumes : database1_volume : You should be able to understand everything here. However, we added two new things: the database1: volumes: directive, and the root volumes: directive. You need to declare your volumes in the root volumes: directive if you want them to be kept persistently. Then, you can bind a volume to a directory in the container. Here, we bind our declared database1_volume to the database1 container's /var/lib/postgresql/data directory. Everything added to this directory will be persistently stored in the volume called database1_volume . So each subsequent run of the container will have access to the previous data! It means you can stop and restart your service without losing the data. OK, let's try it. As we are using Django, we need to \"migrate\" the database first. To do this, we will simply use Docker Compose to start our djangoapp service and run the migration command inside it: docker-compose build # to make sure everything is up-to-date docker-compose run --rm djangoapp /bin/bash -c \"cd hello; ./manage.py migrate\" From now on, it should be really easy to add other databases: just add other database services ( database2 ) with their networks volumes (remember to connect the networks and bind the volumes), update your DATABASES setting in the Django project, and create the environment file for each database in config/db .","title":"Compose: add containers for one or more Postgres databases"},{"location":"posts/docker-compose-django-postgres-nginx/#static-files-collecting-storing-and-serving","text":"UPDATE : a lot of us (including myself) seem to have trouble updating the static files in the designated Docker volume. In retrospect, collecting the static files in the Dockerfile might not be a good idea, because of the caching mechanisms of Docker. A simple and effective solution, mentioned by Isaac T Chikutukutu in the comments, is to remove this command from the Dockerfile and manually run it when needed: docker-compose run djangoapp hello/manage.py collectstatic --no-input I edited the rest of the post accordingly. (End of update) Let's not forget about the static files! In order for NginX to serve them, we will update the config/nginx/conf.d/local.conf file, as well as our docker-compose.yml file. Static files will be stored in volumes. We also need to set the STATIC_ROOT and MEDIA_ROOT variables in the Django project settings. NginX configuration: upstream hello_server { server djangoapp : 8000 ; } server { listen 80 ; server_name localhost ; location / { proxy_pass http://hello_server ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_set_header Host $host ; proxy_redirect off ; } location /static/ { alias /opt/services/djangoapp/static/ ; } location /media/ { alias /opt/services/djangoapp/media/ ; } } Django project settings: # as declared in NginX conf, it must match /opt/services/djangoapp/static/ STATIC_ROOT = os . path . join ( os . path . dirname ( os . path . dirname ( BASE_DIR )), 'static' ) # do the same for media files, it must match /opt/services/djangoapp/media/ MEDIA_ROOT = os . path . join ( os . path . dirname ( os . path . dirname ( BASE_DIR )), 'media' ) Volumes in docker-compose.yml : version : '3' services : djangoapp : build : . volumes : - .:/opt/services/djangoapp/src - static_volume:/opt/services/djangoapp/static # <-- bind the static volume - media_volume:/opt/services/djangoapp/media # <-- bind the media volume networks : - nginx_network - database1_network depends_on : - database1 nginx : image : nginx:1.13 ports : - 8000:80 volumes : - ./config/nginx/conf.d:/etc/nginx/conf.d - static_volume:/opt/services/djangoapp/static # <-- bind the static volume - media_volume:/opt/services/djangoapp/media # <-- bind the media volume depends_on : - djangoapp networks : - nginx_network database1 : image : postgres:10 env_file : - config/db/database1_env networks : - database1_network volumes : - database1_volume:/var/lib/postgresql/data networks : nginx_network : driver : bridge database1_network : driver : bridge volumes : database1_volume : static_volume : # <-- declare the static volume media_volume : # <-- declare the media volume And finally, collect the static files each time you need to update them, by running: docker-compose run djangoapp hello/manage.py collectstatic --no-input","title":"Static files: collecting, storing and serving"},{"location":"posts/docker-compose-django-postgres-nginx/#resources","text":"Here are the resources I used to write this tutorial: Nginx+Flask+Postgres multi-container setup with Docker Compose The repository Docker how to Django + uwsgi/gunicorn + nginx? Django tutorial using Docker, Nginx, Gunicorn and PostgreSQL. Django Development With Docker Compose and Machine Deploy Django, Gunicorn, NGINX, Postgresql using Docker Docker, how to expose a socket over a port for a Django Application And here is the repository that reflects this tutorial (with a few more things). Don't hesitate to share other interesting resources in the comment section!","title":"Resources"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/","text":"When I write a script, I like to have a -h, --help option to help me remember what it does and how it works. But I was never delighted to write this help text. Besides, when your script's options change, you have to update the help text. I also always liked man pages, their search feature and their ability to scroll up and down and not leave any output in the console. But maintaining a man page is even more tedious than maintaining a help text. This is why I thought of using documentation in shell scripts. So I wrote shellman . Writing doc Using shellman An example How it looks Text Man Markdown More Writing doc \u00a4 Shellman can read special comments in your file. These special comments are the documentation of your script or library. They simply begin with two sharps instead of one: # A normal comment. ## A documentation comment. Also, to give a special meaning to your documentation, each documentation line can be tagged, just like you would do in doxygen documentation: ## \\brief A brief description. ## \\desc A long description. ## You can write many lines in this tag. Using shellman \u00a4 After reading and loading all the documentation written in a file, shellman will then be able to output it on stdout (on in another file) in different formats, such as text, man page or markdown. shellman my_script.sh # text output by default man < ( shellman --format man my_script.sh ) # Available formats for -f, --format option: text, man, markdown Shellman also has a --check option that will not output anything, but instead check for written documentation in a file. A simple --check option will return 0 or 1, but display no warnings. Add a --warn option to print warnings on stderr. It will help you fix your documentation, or even do some linting and continuous integration on your scripts' documentation. An example \u00a4 Here is an example of how I write documentation in my scripts now: #!/bin/bash ## \\brief shell script debugger ## \\desc Run a script in path with the -x bash option (and more). ## You should set the PS4 variable for better output. ## \\env PS4 ## Debugger prompt. This is the prefix that bash prepends ## to the current instruction when using -x option. main () { case \" $1 \" in ## \\option -t, --test ## Read the script and warn for encountered syntax errors. ## Do not actually run the script. -t | --test ) FLAGS = -n ; shift ;; ## \\option -v, --verbose ## Run the script with verbose option. -v | --verbose ) FLAGS = -xv ; shift ;; ## \\option -n, --dry-run ## Options test and verbose combined. Validate the syntax ## and print the script to stdout. -n | --dry-run ) FLAGS = -xvn ; shift ;; ## \\option -h, --help ## Print this help and exit. -h | --help ) shellman \" $0 \" ; exit 0 ;; # Here is the black magic! * ) FLAGS = -x ;; esac SCRIPT = $1 shift /bin/bash \" ${ FLAGS } \" \" ${ SCRIPT } \" \" $@ \" } ## \\usage dbg [-tvn] SCRIPT main \" $@ \" How it looks \u00a4 The different outputs would be like the following. Text \u00a4 Usage: dbg [-tvn] SCRIPT Run a script in path with the -x bash option (and more). You should set the PS4 variable for better output. Options: -t, --test Read the script and warn for encountered syntax errors. Do not actually run the script. -v, --verbose Run the script with verbose option. -n, --dry-run Options test and verbose combined. Validate the syntax and print the script to stdout. -h, --help Prints this help and exit. Man \u00a4 Markdown \u00a4 **dbg** - shell script debugger # Usage `dbg [-tvn] SCRIPT` Run a script in path with the -x bash option (and more). You should set the PS4 variable for better output. # Options - `-t, --test`: Read the script and warn for encountered syntax errors. Do not actually run the script. - `-v, --verbose`: Run the script with verbose option. - `-n, --dry-run`: Options test and verbose combined. Validate the syntax and print the script to stdout. - `-h, --help`: Prints this help and exit. # Environment variables - `PS4`: Debugger prompt. This is the prefix that bash prepends to the current instruction when using -x option. ... which would look like: More \u00a4 Shellman supports more tags, in particular: function tags. See its documentation .","title":"Documentation in your shell scripts using shellman"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#writing-doc","text":"Shellman can read special comments in your file. These special comments are the documentation of your script or library. They simply begin with two sharps instead of one: # A normal comment. ## A documentation comment. Also, to give a special meaning to your documentation, each documentation line can be tagged, just like you would do in doxygen documentation: ## \\brief A brief description. ## \\desc A long description. ## You can write many lines in this tag.","title":"Writing doc"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#using-shellman","text":"After reading and loading all the documentation written in a file, shellman will then be able to output it on stdout (on in another file) in different formats, such as text, man page or markdown. shellman my_script.sh # text output by default man < ( shellman --format man my_script.sh ) # Available formats for -f, --format option: text, man, markdown Shellman also has a --check option that will not output anything, but instead check for written documentation in a file. A simple --check option will return 0 or 1, but display no warnings. Add a --warn option to print warnings on stderr. It will help you fix your documentation, or even do some linting and continuous integration on your scripts' documentation.","title":"Using shellman"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#an-example","text":"Here is an example of how I write documentation in my scripts now: #!/bin/bash ## \\brief shell script debugger ## \\desc Run a script in path with the -x bash option (and more). ## You should set the PS4 variable for better output. ## \\env PS4 ## Debugger prompt. This is the prefix that bash prepends ## to the current instruction when using -x option. main () { case \" $1 \" in ## \\option -t, --test ## Read the script and warn for encountered syntax errors. ## Do not actually run the script. -t | --test ) FLAGS = -n ; shift ;; ## \\option -v, --verbose ## Run the script with verbose option. -v | --verbose ) FLAGS = -xv ; shift ;; ## \\option -n, --dry-run ## Options test and verbose combined. Validate the syntax ## and print the script to stdout. -n | --dry-run ) FLAGS = -xvn ; shift ;; ## \\option -h, --help ## Print this help and exit. -h | --help ) shellman \" $0 \" ; exit 0 ;; # Here is the black magic! * ) FLAGS = -x ;; esac SCRIPT = $1 shift /bin/bash \" ${ FLAGS } \" \" ${ SCRIPT } \" \" $@ \" } ## \\usage dbg [-tvn] SCRIPT main \" $@ \"","title":"An example"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#how-it-looks","text":"The different outputs would be like the following.","title":"How it looks"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#text","text":"Usage: dbg [-tvn] SCRIPT Run a script in path with the -x bash option (and more). You should set the PS4 variable for better output. Options: -t, --test Read the script and warn for encountered syntax errors. Do not actually run the script. -v, --verbose Run the script with verbose option. -n, --dry-run Options test and verbose combined. Validate the syntax and print the script to stdout. -h, --help Prints this help and exit.","title":"Text"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#man","text":"","title":"Man"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#markdown","text":"**dbg** - shell script debugger # Usage `dbg [-tvn] SCRIPT` Run a script in path with the -x bash option (and more). You should set the PS4 variable for better output. # Options - `-t, --test`: Read the script and warn for encountered syntax errors. Do not actually run the script. - `-v, --verbose`: Run the script with verbose option. - `-n, --dry-run`: Options test and verbose combined. Validate the syntax and print the script to stdout. - `-h, --help`: Prints this help and exit. # Environment variables - `PS4`: Debugger prompt. This is the prefix that bash prepends to the current instruction when using -x option. ... which would look like:","title":"Markdown"},{"location":"posts/documentation-in-your-shell-scripts-using-shellman/#more","text":"Shellman supports more tags, in particular: function tags. See its documentation .","title":"More"},{"location":"posts/dual-screens-setup-nvidia-bunsenlabs-debian-jessie/","text":"Honestly, this post is mostly a personal memo. I will NOT go through this again! The fun way The quick way ~~If~~ When things go wrong (you should probably read this anyway) Appendix The fun way \u00a4 First, to save your time and mind, install the Linux headers! sudo apt install linux-headers- $( uname -r ) I spent two painful hours this afternoon trying to figure out why it wouldn't work. It happens that I didn't install these headers! Also install these requirements: sudo apt install xserver-xorg xserver-xorg-core xserver-xorg-dev Now, install the Nvidia Detect program. It will tell you where to get the driver from: sudo apt install nvidia-detect Run it: $ nvidia-detect Detected NVIDIA GPUs: 01:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [10de:1402] (rev a1) Your card is only supported by a newer driver that is available in jessie-backports. See http://backports.debian.org for instructions how to use backports. You may also find newer driver packages in experimental. It is recommended to install the nvidia-driver/jessie-backports package. So, in my case, I have to install the driver from Jessie backports. Beware!! It might not be your case! Pay attention to what nvidia-detect says, and do what it says :D! So, for Jessie backports, you would install the driver like this: sudo apt install -t jessie-backports nvidia-driver But the driver is not the only thing you need to install. I'm gonna install the rest from Jessie backports as well, adapt to your needs! sudo apt install -t jessie-backports nvidia-xconfig nvidia-settings xserver-xorg-video-nvidia Reboot! At this point, it won't hurt your OS (it will actually help). Rebooting... Alright, you should now be able to run nvidia-xconfig without trouble: sudo nvidia-xconfig This command will create or update the file /etc/X11/xorg.conf . You can now restart your X server and send me a not-positive comment below if nothing works and your system is broken. sudo systemctl restart lightdm.service Login, then finalize with: sudo nvidia-settings Set your screen(s) resolutions and everything, save the configuration into /etc/X11/xorg.conf , maybe reboot one last time, and you should be good to go! The quick way \u00a4 It's just a summary of the previous commands. Don't actually copy-paste-run it: have a break at the lonely nvidia-detect line. sudo apt install linux-headers- $( uname -r ) sudo apt install xserver-xorg xserver-xorg-core xserver-xorg-dev sudo apt install nvidia-detect nvidia-detect # update the apt commands after this sudo apt install -t jessie-backports nvidia-driver sudo apt install -t jessie-backports nvidia-xconfig nvidia-settings xserver-xorg-video-nvidia # reboot sudo nvidia-xconfig sudo systemctl restart lightdm.service sudo nvidia-settings # update your config and save it # reboot ~~If~~ When things go wrong \u00a4 You should consider following another tutorial. If the X server does not start, hit Ctrl-Alt-F1 to get a non-graphical terminal. From there, simply delete the bad /etc/X11/xorg.conf and run systemctl restart lightdm.service again to get back your beloved GUI. If you installed the wrong things: it's pretty safe to purge all nvidia-* packages, but it is not safe to purge xserver-xorg-* ones . Except the specific xserver-xorg-video-nvidia . Or if you like the Linux (re)installation process. After an update/upgrade : if you upgraded your system with apt update; apt upgrade and your X server does not start anymore, it might be because your Linux headers have not been updated. Remember to always update your Linux headers after the kernel has been upgraded! sudo apt install linux-headers-$(uname -r) Appendix \u00a4 Example of /etc/X11/xorg.conf for dual screen setup: # nvidia-settings: X configuration file generated by nvidia-settings # nvidia-settings: version 375.26 (buildd@debian) Wed Jan 18 14:43:15 UTC 2017 # nvidia-xconfig: X configuration file generated by nvidia-xconfig # nvidia-xconfig: version 340.46 (buildd@brahms) Tue Oct 7 08:00:32 UTC 2014 Section \"ServerLayout\" Identifier \"Layout0\" Screen 0 \"Screen0\" 0 0 InputDevice \"Keyboard0\" \"CoreKeyboard\" InputDevice \"Mouse0\" \"CorePointer\" Option \"Xinerama\" \"0\" EndSection Section \"Files\" EndSection Section \"InputDevice\" # generated from default Identifier \"Mouse0\" Driver \"mouse\" Option \"Protocol\" \"auto\" Option \"Device\" \"/dev/psaux\" Option \"Emulate3Buttons\" \"no\" Option \"ZAxisMapping\" \"4 5\" EndSection Section \"InputDevice\" # generated from default Identifier \"Keyboard0\" Driver \"kbd\" EndSection Section \"Monitor\" Identifier \"Monitor0\" VendorName \"Unknown\" ModelName \"HP LE2202x\" HorizSync 24.0 - 94.0 VertRefresh 50.0 - 76.0 Option \"DPMS\" EndSection Section \"Device\" Identifier \"Device0\" Driver \"nvidia\" VendorName \"NVIDIA Corporation\" BoardName \"GeForce GTX 950\" EndSection Section \"Screen\" # Removed Option \"metamodes\" \"DVI-I-1: nvidia-auto-select +0+0, DVI-D-0: 640x480 +1920+0\" # Removed Option \"metamodes\" \"DVI-I-1: nvidia-auto-select +0+0, DVI-D-0: 1440x900 +1920+128\" Identifier \"Screen0\" Device \"Device0\" Monitor \"Monitor0\" DefaultDepth 24 Option \"Stereo\" \"0\" Option \"metamodes\" \"DVI-I-1: nvidia-auto-select +0+0, DVI-D-0: 1440x900 +1920+90\" Option \"SLI\" \"Off\" Option \"MultiGPU\" \"Off\" Option \"BaseMosaic\" \"off\" SubSection \"Display\" Depth 24 EndSubSection EndSection","title":"How to install NVidia drivers on BunsenLabs/Debian 8 to setup dual screens"},{"location":"posts/dual-screens-setup-nvidia-bunsenlabs-debian-jessie/#the-fun-way","text":"First, to save your time and mind, install the Linux headers! sudo apt install linux-headers- $( uname -r ) I spent two painful hours this afternoon trying to figure out why it wouldn't work. It happens that I didn't install these headers! Also install these requirements: sudo apt install xserver-xorg xserver-xorg-core xserver-xorg-dev Now, install the Nvidia Detect program. It will tell you where to get the driver from: sudo apt install nvidia-detect Run it: $ nvidia-detect Detected NVIDIA GPUs: 01:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [10de:1402] (rev a1) Your card is only supported by a newer driver that is available in jessie-backports. See http://backports.debian.org for instructions how to use backports. You may also find newer driver packages in experimental. It is recommended to install the nvidia-driver/jessie-backports package. So, in my case, I have to install the driver from Jessie backports. Beware!! It might not be your case! Pay attention to what nvidia-detect says, and do what it says :D! So, for Jessie backports, you would install the driver like this: sudo apt install -t jessie-backports nvidia-driver But the driver is not the only thing you need to install. I'm gonna install the rest from Jessie backports as well, adapt to your needs! sudo apt install -t jessie-backports nvidia-xconfig nvidia-settings xserver-xorg-video-nvidia Reboot! At this point, it won't hurt your OS (it will actually help). Rebooting... Alright, you should now be able to run nvidia-xconfig without trouble: sudo nvidia-xconfig This command will create or update the file /etc/X11/xorg.conf . You can now restart your X server and send me a not-positive comment below if nothing works and your system is broken. sudo systemctl restart lightdm.service Login, then finalize with: sudo nvidia-settings Set your screen(s) resolutions and everything, save the configuration into /etc/X11/xorg.conf , maybe reboot one last time, and you should be good to go!","title":"The fun way"},{"location":"posts/dual-screens-setup-nvidia-bunsenlabs-debian-jessie/#the-quick-way","text":"It's just a summary of the previous commands. Don't actually copy-paste-run it: have a break at the lonely nvidia-detect line. sudo apt install linux-headers- $( uname -r ) sudo apt install xserver-xorg xserver-xorg-core xserver-xorg-dev sudo apt install nvidia-detect nvidia-detect # update the apt commands after this sudo apt install -t jessie-backports nvidia-driver sudo apt install -t jessie-backports nvidia-xconfig nvidia-settings xserver-xorg-video-nvidia # reboot sudo nvidia-xconfig sudo systemctl restart lightdm.service sudo nvidia-settings # update your config and save it # reboot","title":"The quick way"},{"location":"posts/dual-screens-setup-nvidia-bunsenlabs-debian-jessie/#if-when-things-go-wrong","text":"You should consider following another tutorial. If the X server does not start, hit Ctrl-Alt-F1 to get a non-graphical terminal. From there, simply delete the bad /etc/X11/xorg.conf and run systemctl restart lightdm.service again to get back your beloved GUI. If you installed the wrong things: it's pretty safe to purge all nvidia-* packages, but it is not safe to purge xserver-xorg-* ones . Except the specific xserver-xorg-video-nvidia . Or if you like the Linux (re)installation process. After an update/upgrade : if you upgraded your system with apt update; apt upgrade and your X server does not start anymore, it might be because your Linux headers have not been updated. Remember to always update your Linux headers after the kernel has been upgraded! sudo apt install linux-headers-$(uname -r)","title":"~~If~~ When things go wrong"},{"location":"posts/dual-screens-setup-nvidia-bunsenlabs-debian-jessie/#appendix","text":"Example of /etc/X11/xorg.conf for dual screen setup: # nvidia-settings: X configuration file generated by nvidia-settings # nvidia-settings: version 375.26 (buildd@debian) Wed Jan 18 14:43:15 UTC 2017 # nvidia-xconfig: X configuration file generated by nvidia-xconfig # nvidia-xconfig: version 340.46 (buildd@brahms) Tue Oct 7 08:00:32 UTC 2014 Section \"ServerLayout\" Identifier \"Layout0\" Screen 0 \"Screen0\" 0 0 InputDevice \"Keyboard0\" \"CoreKeyboard\" InputDevice \"Mouse0\" \"CorePointer\" Option \"Xinerama\" \"0\" EndSection Section \"Files\" EndSection Section \"InputDevice\" # generated from default Identifier \"Mouse0\" Driver \"mouse\" Option \"Protocol\" \"auto\" Option \"Device\" \"/dev/psaux\" Option \"Emulate3Buttons\" \"no\" Option \"ZAxisMapping\" \"4 5\" EndSection Section \"InputDevice\" # generated from default Identifier \"Keyboard0\" Driver \"kbd\" EndSection Section \"Monitor\" Identifier \"Monitor0\" VendorName \"Unknown\" ModelName \"HP LE2202x\" HorizSync 24.0 - 94.0 VertRefresh 50.0 - 76.0 Option \"DPMS\" EndSection Section \"Device\" Identifier \"Device0\" Driver \"nvidia\" VendorName \"NVIDIA Corporation\" BoardName \"GeForce GTX 950\" EndSection Section \"Screen\" # Removed Option \"metamodes\" \"DVI-I-1: nvidia-auto-select +0+0, DVI-D-0: 640x480 +1920+0\" # Removed Option \"metamodes\" \"DVI-I-1: nvidia-auto-select +0+0, DVI-D-0: 1440x900 +1920+128\" Identifier \"Screen0\" Device \"Device0\" Monitor \"Monitor0\" DefaultDepth 24 Option \"Stereo\" \"0\" Option \"metamodes\" \"DVI-I-1: nvidia-auto-select +0+0, DVI-D-0: 1440x900 +1920+90\" Option \"SLI\" \"Off\" Option \"MultiGPU\" \"Off\" Option \"BaseMosaic\" \"off\" SubSection \"Display\" Depth 24 EndSubSection EndSection","title":"Appendix"},{"location":"posts/python-static-code-analysis-tools/","text":"Python static code analysis is often used in Continuous Integration. I use it in every Python or Django package I develop, into a Tox configuration. The difficult part is to choose the right tools, because there are many, and to configure them correctly. Thankfully, there also are tools using other tools to reduce the amount of configuration we have to do. This ends up in a big mix of tools, which is represented in the following chart. In this post, we will see the advantages and disadvantages of these tools, and compare them a bit. Chart realized with Draw.io. The \"aggregator\" tools \u00a4 Some tools have a unique purpose, while others aim at doing more. This is the case of Prospector , Pylama , Pylint and Flake8 . I see two types of aggregators here: the ones that do everything themselves (mostly): Pylint , and the ones that combine the result of many smaller tools: Prospector , Pylama and Flake8 . What is funny here is that both Prospector and Pylama use not only tools used by Pylint , but Pylint as well! There is a bit of overlap here. Pylint \u00a4 What I like in Pylint is its ability to rate your code, and keep a record of its evolution, telling you if it has improved or regressed between each run. I also love it's Similar tool, a one of its kind, used to output the similarities in your code. You can run it with python -m pylint.checkers.similar FILES . It's very fast, but will start to take some time with a lot of files. Pylama \u00a4 Pylama seems not to have much activity since last year and has unresolved issues and open pull requests. At the time of writing, it's Pydocstyle integration is broken. I also tried using it from Python because I wanted to count the number of issues in some code, and was unsuccessful to do so. Still, I used Pylama for some time because I liked a lot how it can be configured, directly in setup.cfg . Its options are simple to use and simple to configure. I find ignoring warnings in Pylama much more easier than in Prospector , as they are all ignored on the same line, and are consistent between them (only names composed of a letter and a code). Pylama can also be deactivated in specific locations of your code with comments like # pylama:ignore=D105,D106 , and recognizes # noqa comments. Prospector \u00a4 Prospector is my favorite here: I used it for quite some time before switching to Pylama , and I'm back with it now. I was afraid that it was deperishing, because maintainers did not seem to answer pull requests and fix broken integration, but many fixes have been pushed now. I think it's a great project and it deserves more activity! What I like about Prospector is that it uses many tools to check many things, and it does it well: as I said earlier, there is some overlap between tools; Prospector handles that by keeping a record of similar warnings and only keep one of each to avoid redundancy in its reports! This is not done by default in Pylama , so you have to manually ignore redundant warnings in its configuration. Prospector also helped me discover tools I never heard of before, like Dodgy , Vulture , Pyroma and Frosted . I would like to see integrations for Radon (used by Pylama ) as it offers the same thing as McCabe plus other stuff, as well as for Bandit which is a Python code security checker. What I miss in Prospector is the ability to configure it directly in setup.cfg , which I enjoyed a lot with Pylama . This is another story but I am trying hard to reduce the number of configuration files in my projects. I think tools should be able to find their configuration files into a specific location of your project, not only in the root directory, where configuration files pile up. I would be very happy to see some sort of standard for configuration files location rising, just as what EditorConfig is doing. Maybe a ConfigConfig standard? With a .configconfig file at the root of the repository telling where to find the configuraton files, and tools being able to read it, or by default search at the root or in the config folder? Flake8 \u00a4 I don't have much to say about Flake8 here. I did try it some time ago, but since Prospector does what it does, I won't use it. But it's only my personal choice and it can be a really good choice for others if they don't need to run all the possible code analysis tools in the world (unlike me :D). The \"do-one-thing-but-do-it-well\" tools \u00a4 Work in progress, coming soon! Meanwhile, please share your thoughts! Did I say something wrong? Did I forget great tools? Did I write \"tools\" to much? I will gladly update the post with your sharings! Also worth noticing \u00a4 Go check Safety from PyUp.io , which check your requirements (dependencies) for known-vulnerabilities! Also Pyt for which I didn't take enough time to understand what it does and how it works, and great Code Quality web-services like Landscape.io and Codacy ! Links: \u00a4 Bandit: https://github.com/openstack/bandit Codacy: https://www.codacy.com/ Dodgy: https://github.com/landscapeio/dodgy Draw.io: https://www.draw.io/ Flake8: https://github.com/PyCQA/flake8 Frosted: https://github.com/timothycrosley/frosted Landscape.io: https://landscape.io/ McCabe: https://github.com/PyCQA/mccabe Prospector: https://github.com/landscapeio/prospector Pydocstyle: https://github.com/PyCQA/pydocstyle Pylama: https://github.com/klen/pylama Pylint: https://github.com/PyCQA/pylint Pyroma: https://github.com/regebro/pyroma Pyt: https://github.com/python-security/pyt PyUp.io: https://pyup.io/ Radon: https://github.com/rubik/radon Safety: https://github.com/pyupio/safety Tox: https://github.com/tox-dev/tox Vulture: https://github.com/jendrikseipp/vulture","title":"Python static code analysis tools"},{"location":"posts/python-static-code-analysis-tools/#the-aggregator-tools","text":"Some tools have a unique purpose, while others aim at doing more. This is the case of Prospector , Pylama , Pylint and Flake8 . I see two types of aggregators here: the ones that do everything themselves (mostly): Pylint , and the ones that combine the result of many smaller tools: Prospector , Pylama and Flake8 . What is funny here is that both Prospector and Pylama use not only tools used by Pylint , but Pylint as well! There is a bit of overlap here.","title":"The \"aggregator\" tools"},{"location":"posts/python-static-code-analysis-tools/#pylint","text":"What I like in Pylint is its ability to rate your code, and keep a record of its evolution, telling you if it has improved or regressed between each run. I also love it's Similar tool, a one of its kind, used to output the similarities in your code. You can run it with python -m pylint.checkers.similar FILES . It's very fast, but will start to take some time with a lot of files.","title":"Pylint"},{"location":"posts/python-static-code-analysis-tools/#pylama","text":"Pylama seems not to have much activity since last year and has unresolved issues and open pull requests. At the time of writing, it's Pydocstyle integration is broken. I also tried using it from Python because I wanted to count the number of issues in some code, and was unsuccessful to do so. Still, I used Pylama for some time because I liked a lot how it can be configured, directly in setup.cfg . Its options are simple to use and simple to configure. I find ignoring warnings in Pylama much more easier than in Prospector , as they are all ignored on the same line, and are consistent between them (only names composed of a letter and a code). Pylama can also be deactivated in specific locations of your code with comments like # pylama:ignore=D105,D106 , and recognizes # noqa comments.","title":"Pylama"},{"location":"posts/python-static-code-analysis-tools/#prospector","text":"Prospector is my favorite here: I used it for quite some time before switching to Pylama , and I'm back with it now. I was afraid that it was deperishing, because maintainers did not seem to answer pull requests and fix broken integration, but many fixes have been pushed now. I think it's a great project and it deserves more activity! What I like about Prospector is that it uses many tools to check many things, and it does it well: as I said earlier, there is some overlap between tools; Prospector handles that by keeping a record of similar warnings and only keep one of each to avoid redundancy in its reports! This is not done by default in Pylama , so you have to manually ignore redundant warnings in its configuration. Prospector also helped me discover tools I never heard of before, like Dodgy , Vulture , Pyroma and Frosted . I would like to see integrations for Radon (used by Pylama ) as it offers the same thing as McCabe plus other stuff, as well as for Bandit which is a Python code security checker. What I miss in Prospector is the ability to configure it directly in setup.cfg , which I enjoyed a lot with Pylama . This is another story but I am trying hard to reduce the number of configuration files in my projects. I think tools should be able to find their configuration files into a specific location of your project, not only in the root directory, where configuration files pile up. I would be very happy to see some sort of standard for configuration files location rising, just as what EditorConfig is doing. Maybe a ConfigConfig standard? With a .configconfig file at the root of the repository telling where to find the configuraton files, and tools being able to read it, or by default search at the root or in the config folder?","title":"Prospector"},{"location":"posts/python-static-code-analysis-tools/#flake8","text":"I don't have much to say about Flake8 here. I did try it some time ago, but since Prospector does what it does, I won't use it. But it's only my personal choice and it can be a really good choice for others if they don't need to run all the possible code analysis tools in the world (unlike me :D).","title":"Flake8"},{"location":"posts/python-static-code-analysis-tools/#the-do-one-thing-but-do-it-well-tools","text":"Work in progress, coming soon! Meanwhile, please share your thoughts! Did I say something wrong? Did I forget great tools? Did I write \"tools\" to much? I will gladly update the post with your sharings!","title":"The \"do-one-thing-but-do-it-well\" tools"},{"location":"posts/python-static-code-analysis-tools/#also-worth-noticing","text":"Go check Safety from PyUp.io , which check your requirements (dependencies) for known-vulnerabilities! Also Pyt for which I didn't take enough time to understand what it does and how it works, and great Code Quality web-services like Landscape.io and Codacy !","title":"Also worth noticing"},{"location":"posts/python-static-code-analysis-tools/#links","text":"Bandit: https://github.com/openstack/bandit Codacy: https://www.codacy.com/ Dodgy: https://github.com/landscapeio/dodgy Draw.io: https://www.draw.io/ Flake8: https://github.com/PyCQA/flake8 Frosted: https://github.com/timothycrosley/frosted Landscape.io: https://landscape.io/ McCabe: https://github.com/PyCQA/mccabe Prospector: https://github.com/landscapeio/prospector Pydocstyle: https://github.com/PyCQA/pydocstyle Pylama: https://github.com/klen/pylama Pylint: https://github.com/PyCQA/pylint Pyroma: https://github.com/regebro/pyroma Pyt: https://github.com/python-security/pyt PyUp.io: https://pyup.io/ Radon: https://github.com/rubik/radon Safety: https://github.com/pyupio/safety Tox: https://github.com/tox-dev/tox Vulture: https://github.com/jendrikseipp/vulture","title":"Links:"},{"location":"posts/save-pytest-logs-as-artifact-gitlab-ci/","text":"While I was writing tests for one of my latest project, aria2p , I noticed that some tests that were passing on my local machine were now failing on the GitLab CI runner. I decided I needed to write all exceptions to log files so I could inspect what happened on GitLab. Since these tests were for the interactive interface of aria2p , using asciimatics and therefore an alternate buffer, I could not add print statements in the code to generate useful debug information that would be printed in the GitLab job logs. But I was already logging every exception! I just had to enable the logging functionality while running tests, and write them to the disk so I could create an artifact! The artifact could then be downloaded, and inspected in great details to see exactly what happened. What I use: loguru as a logging utility. It's really easy to use, and I have to admit that I just like the colors pytest to run the test suite. It's very powerful and has lots of nice plugins, like pytest-cov , pytest-sugar and pytest-xdist . GitLab CI as Continuous Integration service. Logging with loguru \u00a4 Let say you have an interactive function that accepts user input and execute code according to this input. Anything could happen, right? So you log every exception that might happen like this: from loguru import logger def run (): try : ... # actual code except Exception as error : logger . exception ( error ) Testing the code \u00a4 Now in pytest , you would write a test function: from my_package import run def test_run (): interface = run () # verify things afterwards assert interface . state == 1 This code is not really important. What we're interested in is how we will write the logs of each test function into its own file on the disk. Writing logs for each test \u00a4 pytest allows you to run code before each test using fixtures. It's a bit more powerful than that (check the docs), but this is how we use it in this post. In your tests/ directory, add a conftest.py file that pytest will automatically use. Our fixture is written like this: from pathlib import Path from loguru import logger import pytest @pytest . fixture ( autouse = True ) def write_logs ( request ): # put logs in tests/logs log_path = Path ( \"tests\" ) / \"logs\" # tidy logs in subdirectories based on test module and class names module = request . module class_ = request . cls name = request . node . name + \".log\" if module : log_path /= module . __name__ . replace ( \"tests.\" , \"\" ) if class_ : log_path /= class_ . __name__ log_path . mkdir ( parents = True , exist_ok = True ) # append last part of the name log_path /= name # enable the logger logger . remove () logger . configure ( handlers = [{ \"sink\" : log_path , \"level\" : \"TRACE\" , \"mode\" : \"w\" }]) logger . enable ( \"my_package\" ) Let's explain this code in details. - @pytest.fixture(autouse=True) : this decorator tells to always run the declared function before a test. - def write_logs(request): : the request parameter actually tells pytest to use its built-in request fixture! It allows to inspect the context of the current test function, the one that \"requested\" this fixture. We will retrieve the module, class and function name from the request object. - log_path = Path(\"tests\") / \"logs\" : loguru accepts File-like objects, isn't it nice? We will write our tests in the subdirectory \"tests/logs\". - then we simply build the file path using the module name if it exists, the class name if it exists, and create the parents directory, ignoring errors if they already exists. - logger.remove() : loguru starts with a default handler. We remove it before adding our own. - we set our handler using the log_path variable, the TRACE level (the lowest level to catch everything), and the mode \"w\" . The default mode is \"a\" , for append, but we want don't want to keep the logs of previous runs, so we overwrite them each time. - finally, enable the logger for your package. We're setup. With this tests structure: tests/ \u251c\u2500\u2500 test_cli.py \u2502 \u251c\u2500\u2500 def test_a \u2502 \u2514\u2500\u2500 def test_b \u2514\u2500\u2500 test_interface.py \u251c\u2500\u2500 class TestFirstClass \u2502 \u251c\u2500\u2500 def test_a \u2502 \u2514\u2500\u2500 def test_b \u2514\u2500\u2500 class TestSecondClass \u2502 \u251c\u2500\u2500 def test_a \u2502 \u2514\u2500\u2500 def test_b \u2514\u2500\u2500 def test_c You will have this logs tree: tests/ \u2514\u2500\u2500 logs/ \u251c\u2500\u2500 test_cli/ \u2502 \u251c\u2500\u2500 test_a.log \u2502 \u2514\u2500\u2500 test_b.log \u2514\u2500\u2500 test_interface/ \u251c\u2500\u2500 TestFirstClass/ \u2502 \u251c\u2500\u2500 test_a.log \u2502 \u2514\u2500\u2500 test_b.log \u2514\u2500\u2500 TestSecondClass/ \u2502 \u251c\u2500\u2500 test_a.log \u2502 \u2514\u2500\u2500 test_b.log \u2514\u2500\u2500 test_c.log OK great! Now let's configure GitLab CI to use this directory as an artifact. Logs artifact on GitLab CI \u00a4 You'll typically want to download the logs when a job failed. Add this in your test job: artifacts : paths : - tests/logs when : on_failure expire_in : 1 week Now when a job fails, you'll be able to download or browse the logs: Or even from the pipelines pages:","title":"Save the logs generated during a pytest run as a job artifact on GitLab CI"},{"location":"posts/save-pytest-logs-as-artifact-gitlab-ci/#logging-with-loguru","text":"Let say you have an interactive function that accepts user input and execute code according to this input. Anything could happen, right? So you log every exception that might happen like this: from loguru import logger def run (): try : ... # actual code except Exception as error : logger . exception ( error )","title":"Logging with loguru"},{"location":"posts/save-pytest-logs-as-artifact-gitlab-ci/#testing-the-code","text":"Now in pytest , you would write a test function: from my_package import run def test_run (): interface = run () # verify things afterwards assert interface . state == 1 This code is not really important. What we're interested in is how we will write the logs of each test function into its own file on the disk.","title":"Testing the code"},{"location":"posts/save-pytest-logs-as-artifact-gitlab-ci/#writing-logs-for-each-test","text":"pytest allows you to run code before each test using fixtures. It's a bit more powerful than that (check the docs), but this is how we use it in this post. In your tests/ directory, add a conftest.py file that pytest will automatically use. Our fixture is written like this: from pathlib import Path from loguru import logger import pytest @pytest . fixture ( autouse = True ) def write_logs ( request ): # put logs in tests/logs log_path = Path ( \"tests\" ) / \"logs\" # tidy logs in subdirectories based on test module and class names module = request . module class_ = request . cls name = request . node . name + \".log\" if module : log_path /= module . __name__ . replace ( \"tests.\" , \"\" ) if class_ : log_path /= class_ . __name__ log_path . mkdir ( parents = True , exist_ok = True ) # append last part of the name log_path /= name # enable the logger logger . remove () logger . configure ( handlers = [{ \"sink\" : log_path , \"level\" : \"TRACE\" , \"mode\" : \"w\" }]) logger . enable ( \"my_package\" ) Let's explain this code in details. - @pytest.fixture(autouse=True) : this decorator tells to always run the declared function before a test. - def write_logs(request): : the request parameter actually tells pytest to use its built-in request fixture! It allows to inspect the context of the current test function, the one that \"requested\" this fixture. We will retrieve the module, class and function name from the request object. - log_path = Path(\"tests\") / \"logs\" : loguru accepts File-like objects, isn't it nice? We will write our tests in the subdirectory \"tests/logs\". - then we simply build the file path using the module name if it exists, the class name if it exists, and create the parents directory, ignoring errors if they already exists. - logger.remove() : loguru starts with a default handler. We remove it before adding our own. - we set our handler using the log_path variable, the TRACE level (the lowest level to catch everything), and the mode \"w\" . The default mode is \"a\" , for append, but we want don't want to keep the logs of previous runs, so we overwrite them each time. - finally, enable the logger for your package. We're setup. With this tests structure: tests/ \u251c\u2500\u2500 test_cli.py \u2502 \u251c\u2500\u2500 def test_a \u2502 \u2514\u2500\u2500 def test_b \u2514\u2500\u2500 test_interface.py \u251c\u2500\u2500 class TestFirstClass \u2502 \u251c\u2500\u2500 def test_a \u2502 \u2514\u2500\u2500 def test_b \u2514\u2500\u2500 class TestSecondClass \u2502 \u251c\u2500\u2500 def test_a \u2502 \u2514\u2500\u2500 def test_b \u2514\u2500\u2500 def test_c You will have this logs tree: tests/ \u2514\u2500\u2500 logs/ \u251c\u2500\u2500 test_cli/ \u2502 \u251c\u2500\u2500 test_a.log \u2502 \u2514\u2500\u2500 test_b.log \u2514\u2500\u2500 test_interface/ \u251c\u2500\u2500 TestFirstClass/ \u2502 \u251c\u2500\u2500 test_a.log \u2502 \u2514\u2500\u2500 test_b.log \u2514\u2500\u2500 TestSecondClass/ \u2502 \u251c\u2500\u2500 test_a.log \u2502 \u2514\u2500\u2500 test_b.log \u2514\u2500\u2500 test_c.log OK great! Now let's configure GitLab CI to use this directory as an artifact.","title":"Writing logs for each test"},{"location":"posts/save-pytest-logs-as-artifact-gitlab-ci/#logs-artifact-on-gitlab-ci","text":"You'll typically want to download the logs when a job failed. Add this in your test job: artifacts : paths : - tests/logs when : on_failure expire_in : 1 week Now when a job fails, you'll be able to download or browse the logs: Or even from the pipelines pages:","title":"Logs artifact on GitLab CI"},{"location":"posts/unify-logging-for-a-gunicorn-uvicorn-app/","text":"I recently started playing with FastAPI and HTTPX , and I am deploying my app with Gunicorn and Uvicorn workers. But when serving, the logs from each component looks quite different from the others. I want them to all look the same, so I can easily read them or exploit them in something like Kibana . After a lot of hours trying to understand how Python logging works, and how to override libraries' logging settings, here is what I have... A single run.py file! I didn't want to split logging configuration, Gunicorn configuration, and the rest of the code into multiple files, as it was harder to wrap my head around it. Everything is contained in this single file: import os import logging import sys from gunicorn.app.base import BaseApplication from gunicorn.glogging import Logger from loguru import logger from my_app.app import app LOG_LEVEL = logging . getLevelName ( os . environ . get ( \"LOG_LEVEL\" , \"DEBUG\" )) JSON_LOGS = True if os . environ . get ( \"JSON_LOGS\" , \"0\" ) == \"1\" else False WORKERS = int ( os . environ . get ( \"GUNICORN_WORKERS\" , \"5\" )) class InterceptHandler ( logging . Handler ): def emit ( self , record ): # Get corresponding Loguru level if it exists try : level = logger . level ( record . levelname ) . name except ValueError : level = record . levelno # Find caller from where originated the logged message frame , depth = logging . currentframe (), 2 while frame . f_code . co_filename == logging . __file__ : frame = frame . f_back depth += 1 logger . opt ( depth = depth , exception = record . exc_info ) . log ( level , record . getMessage ()) class StubbedGunicornLogger ( Logger ): def setup ( self , cfg ): handler = logging . NullHandler () self . error_logger = logging . getLogger ( \"gunicorn.error\" ) self . error_logger . addHandler ( handler ) self . access_logger = logging . getLogger ( \"gunicorn.access\" ) self . access_logger . addHandler ( handler ) self . error_log . setLevel ( LOG_LEVEL ) self . access_log . setLevel ( LOG_LEVEL ) class StandaloneApplication ( BaseApplication ): \"\"\"Our Gunicorn application.\"\"\" def __init__ ( self , app , options = None ): self . options = options or {} self . application = app super () . __init__ () def load_config ( self ): config = { key : value for key , value in self . options . items () if key in self . cfg . settings and value is not None } for key , value in config . items (): self . cfg . set ( key . lower (), value ) def load ( self ): return self . application if __name__ == '__main__' : intercept_handler = InterceptHandler () # logging.basicConfig(handlers=[intercept_handler], level=LOG_LEVEL) # logging.root.handlers = [intercept_handler] logging . root . setLevel ( LOG_LEVEL ) seen = set () for name in [ * logging . root . manager . loggerDict . keys (), \"gunicorn\" , \"gunicorn.access\" , \"gunicorn.error\" , \"uvicorn\" , \"uvicorn.access\" , \"uvicorn.error\" , ]: if name not in seen : seen . add ( name . split ( \".\" )[ 0 ]) logging . getLogger ( name ) . handlers = [ intercept_handler ] logger . configure ( handlers = [{ \"sink\" : sys . stdout , \"serialize\" : JSON_LOGS }]) options = { \"bind\" : \"0.0.0.0\" , \"workers\" : WORKERS , \"accesslog\" : \"-\" , \"errorlog\" : \"-\" , \"worker_class\" : \"uvicorn.workers.UvicornWorker\" , \"logger_class\" : StubbedGunicornLogger } StandaloneApplication ( app , options ) . run () If you are in a hurry, copy-paste it, change the Gunicorn options at the end, and try it! If you're not, I will explain each part below. import os import logging import sys from gunicorn.app.base import BaseApplication from gunicorn.glogging import Logger from loguru import logger This part is easy, we simply import the things we need. The Gunicorn BaseApplication so we can run Gunicorn directly from this script, and its Logger that we will override a bit. We are using Loguru later in the code, to have a pretty log format, or to serialize them. from my_app.app import app In my project, I have a my_app package with an app module. My FastAPI application is declared in this module, something like app = FastAPI() . LOG_LEVEL = logging . getLevelName ( os . environ . get ( \"LOG_LEVEL\" , \"DEBUG\" )) JSON_LOGS = True if os . environ . get ( \"JSON_LOGS\" , \"0\" ) == \"1\" else False WORKERS = int ( os . environ . get ( \"GUNICORN_WORKERS\" , \"5\" )) We setup some values from environment variables, useful for development vs. production setups. JSON_LOGS tells if we should serialize the logs to JSON, and WORKERS tells how many workers we want to have. class InterceptHandler ( logging . Handler ): def emit ( self , record ): # Get corresponding Loguru level if it exists try : level = logger . level ( record . levelname ) . name except ValueError : level = record . levelno # Find caller from where originated the logged message frame , depth = logging . currentframe (), 2 while frame . f_code . co_filename == logging . __file__ : frame = frame . f_back depth += 1 logger . opt ( depth = depth , exception = record . exc_info ) . log ( level , record . getMessage ()) This code is copy-pasted from Loguru's documentation! This handler will be used to intercept the logs emitted by libraries and re-emit them through Loguru. class StubbedGunicornLogger ( Logger ): def setup ( self , cfg ): handler = logging . NullHandler () self . error_logger = logging . getLogger ( \"gunicorn.error\" ) self . error_logger . addHandler ( handler ) self . access_logger = logging . getLogger ( \"gunicorn.access\" ) self . access_logger . addHandler ( handler ) self . error_log . setLevel ( LOG_LEVEL ) self . access_log . setLevel ( LOG_LEVEL ) This code was copied from this GitHub comment by @dcosson . Thanks! It will allow us to override Gunicorn's own logging configuration so its logs can be formatted like the rest. I'm not sure about the last two lines, as removing them doesn't change anything. There are still mysteries about Python logging that I couldn't resolve... class StandaloneApplication ( BaseApplication ): \"\"\"Our Gunicorn application.\"\"\" def __init__ ( self , app , options = None ): self . options = options or {} self . application = app super () . __init__ () def load_config ( self ): config = { key : value for key , value in self . options . items () if key in self . cfg . settings and value is not None } for key , value in config . items (): self . cfg . set ( key . lower (), value ) def load ( self ): return self . application This code is taken from Gunicorn's documentation . We declare a simple Gunicorn application that we will be able to run. It accepts all Gunicorn's options. if __name__ == '__main__' : intercept_handler = InterceptHandler () # logging.basicConfig(handlers=[intercept_handler], level=LOG_LEVEL) # logging.root.handlers = [intercept_handler] logging . root . setLevel ( LOG_LEVEL ) We simply instantiate our interception handler, and set the log level on the root logger. Once again, I fail to understand how this works exactly, as the two commented lines have no impact on the result. I did a lot of trial and error and ended up with something working, but I cannot entirely explain why. The idea here was to set the handler on the root logger so it intercepts everything, but it was not enough (logs were not all intercepted). seen = set () for name in [ * logging . root . manager . loggerDict . keys (), \"gunicorn\" , \"gunicorn.access\" , \"gunicorn.error\" , \"uvicorn\" , \"uvicorn.access\" , \"uvicorn.error\" , ]: if name not in seen : seen . add ( name . split ( \".\" )[ 0 ]) logging . getLogger ( name ) . handlers = [ intercept_handler ] Here we iterate on all the possible loggers declared by libraries to override their handlers with our interception handler. This is where we actually configure every logger to behave the same. For a reason that I fail to understand, Gunicorn and Uvicorn do not appear in the root logger manager, so we have to hardcode them in the list. We also use a set to avoid setting the interception handler on the parent of a logger that is already configured, because otherwise logs would be emitted twice or more. I'm not sure this code can handle levels of nested loggers deeper than two. logger . configure ( handlers = [{ \"sink\" : sys . stdout , \"serialize\" : JSON_LOGS }]) Here we configure Loguru to write on the standard output, and to serialize logs if needed. At some point I was also using activation=[(\"\", True)] ( see Loguru's docs ), but it seems it's not required either. options = { \"bind\" : \"0.0.0.0\" , \"workers\" : WORKERS , \"accesslog\" : \"-\" , \"errorlog\" : \"-\" , \"worker_class\" : \"uvicorn.workers.UvicornWorker\" , \"logger_class\" : StubbedGunicornLogger } StandaloneApplication ( app , options ) . run () Finally, we set our Gunicorn options, wiring things up, and run our application! Well, I'm not really proud of this code, but it works!","title":"Unify Python logging for a Gunicorn/Uvicorn/FastAPI application"},{"location":"posts/write-and-use-a-tox-plugin-from-inside-your-package/","text":"So let's say you wrote a program that is using tox and you want to add some options to tox's command-line. Reading at tox's documentation about plugins, you see that you have to make a pip installable plugin and setup some entry point. In fact, you can skip the pip installable side and integrate the plugin directly within your code. It can be done in three very simple steps. Add a tox_module in your package Setup the entry points Update your code Add a tox_module in your package \u00a4 Create a new module somewhere. Its name should begin with tox_ . # tox_your_module.py from tox import hookimpl @hookimpl def tox_addoption ( parser ): parser . add_argument ( 'my_new_option' ) # read argparse's doc online to see more complex examples Setup the entry points \u00a4 Tox has an automagic system that will see installed plugins and load them when you call it. Just set the new tox entry point: # setup.py setup ( ... entry_points = { 'console_scripts' : 'your_program = your_program.main:main' , # to be adapted 'tox' : [ 'your_module = your_program.tox_your_module' ] }, ... ) Installing your package will make it visible to tox. Update your code \u00a4 In your code you will now be able to use the my_new_option option in tox's config object, and do whatever you want with it! from tox.session import prepare config = prepare ( args ) print ( config . option . my_new_option ) Voil\u00e0. Of course your plugin could be much more complex, but this is another story.","title":"Write and use a tox plugin from inside your package"},{"location":"posts/write-and-use-a-tox-plugin-from-inside-your-package/#add-a-tox_module-in-your-package","text":"Create a new module somewhere. Its name should begin with tox_ . # tox_your_module.py from tox import hookimpl @hookimpl def tox_addoption ( parser ): parser . add_argument ( 'my_new_option' ) # read argparse's doc online to see more complex examples","title":"Add a tox_module in your package"},{"location":"posts/write-and-use-a-tox-plugin-from-inside-your-package/#setup-the-entry-points","text":"Tox has an automagic system that will see installed plugins and load them when you call it. Just set the new tox entry point: # setup.py setup ( ... entry_points = { 'console_scripts' : 'your_program = your_program.main:main' , # to be adapted 'tox' : [ 'your_module = your_program.tox_your_module' ] }, ... ) Installing your package will make it visible to tox.","title":"Setup the entry points"},{"location":"posts/write-and-use-a-tox-plugin-from-inside-your-package/#update-your-code","text":"In your code you will now be able to use the my_new_option option in tox's config object, and do whatever you want with it! from tox.session import prepare config = prepare ( args ) print ( config . option . my_new_option ) Voil\u00e0. Of course your plugin could be much more complex, but this is another story.","title":"Update your code"},{"location":"showcase/aria2p/","text":"aria2p \u00a4 Repository: https://github.com/pawamoy/aria2p Documentation: https://pawamoy.github.io/aria2p I like automating things, or at least making them scriptable, controllable from the command line. One of those \"things\" is downloading stuff from the internet. I tried some GUI applications for torrents (Transmission, Deluge), and was always disappointed by their lack of CLI-friendliness. On r/torrents , people pointed me to Rtorrent (or is it RTorrent? rTorrent?), which seems to be extremely configurable, scriptable, and powerful. But it also seemed very complicated to grasp, particularly because it implements its own configuration language. So here I went again... doing things myself. Well, not really myself. I chose to write a client for aria2 , because it offered an XML-RPC and a JSON-RPC interface. And this is how aria2p was born! I started by creating a client with nothing more, nothing less than what aria2 exposes in its JSON-RPC interface. And then I built upon that client to provide more high-level methods. The manual page of aria2 is really well detailed, so it helped a lot. I'm now running it on a RaspberryPi on which a hard-drive is plugged. I can define callbacks (as Python functions) to process a download's files when it finishes, like moving them in the right directory based on their extension. I really enjoyed writing this library and command-line tool, and I poured all my experience in Python in this project's management and configuration. I always use it as my sandbox for new cutting edge analysis tools or static site generators. I also learned how to write an HTOP-like interface, something I always admired and wanted to try coding. There were some challenges, like managing vertical+horizontal scroll while following a particular line or not, as well as separating the data from the logic and the presentation, or making the interface snappy, reacting fast to user input, without loosing them. It's not finished though! There is room for a lot of improvements, like charts at the top of the window, a status bar, additional views for download details, configuration option, more actions, etc. I want to write a tutorial on how to write an HTOP-like interface in Python someday, using aria2p as an example. Such a tutorial would have helped me tremendously back then, so I'm sure it would help other developers.","title":"aria2p"},{"location":"showcase/aria2p/#aria2p","text":"Repository: https://github.com/pawamoy/aria2p Documentation: https://pawamoy.github.io/aria2p I like automating things, or at least making them scriptable, controllable from the command line. One of those \"things\" is downloading stuff from the internet. I tried some GUI applications for torrents (Transmission, Deluge), and was always disappointed by their lack of CLI-friendliness. On r/torrents , people pointed me to Rtorrent (or is it RTorrent? rTorrent?), which seems to be extremely configurable, scriptable, and powerful. But it also seemed very complicated to grasp, particularly because it implements its own configuration language. So here I went again... doing things myself. Well, not really myself. I chose to write a client for aria2 , because it offered an XML-RPC and a JSON-RPC interface. And this is how aria2p was born! I started by creating a client with nothing more, nothing less than what aria2 exposes in its JSON-RPC interface. And then I built upon that client to provide more high-level methods. The manual page of aria2 is really well detailed, so it helped a lot. I'm now running it on a RaspberryPi on which a hard-drive is plugged. I can define callbacks (as Python functions) to process a download's files when it finishes, like moving them in the right directory based on their extension. I really enjoyed writing this library and command-line tool, and I poured all my experience in Python in this project's management and configuration. I always use it as my sandbox for new cutting edge analysis tools or static site generators. I also learned how to write an HTOP-like interface, something I always admired and wanted to try coding. There were some challenges, like managing vertical+horizontal scroll while following a particular line or not, as well as separating the data from the logic and the presentation, or making the interface snappy, reacting fast to user input, without loosing them. It's not finished though! There is room for a lot of improvements, like charts at the top of the window, a status bar, additional views for download details, configuration option, more actions, etc. I want to write a tutorial on how to write an HTOP-like interface in Python someday, using aria2p as an example. Such a tutorial would have helped me tremendously back then, so I'm sure it would help other developers.","title":"aria2p"},{"location":"showcase/copier-poetry/","text":"copier-poetry \u00a4 Repository: https://github.com/pawamoy/copier-poetry","title":"copier-poetry"},{"location":"showcase/copier-poetry/#copier-poetry","text":"Repository: https://github.com/pawamoy/copier-poetry","title":"copier-poetry"},{"location":"showcase/dependenpy/","text":"dependenpy \u00a4 Repository: https://github.com/pawamoy/dependenpy Documentation: https://dependenpy.readthedocs.io/en/latest/index.html","title":"dependenpy"},{"location":"showcase/dependenpy/#dependenpy","text":"Repository: https://github.com/pawamoy/dependenpy Documentation: https://dependenpy.readthedocs.io/en/latest/index.html","title":"dependenpy"},{"location":"showcase/failprint/","text":"failprint \u00a4 Repository: https://github.com/pawamoy/failprint Documentation: https://pawamoy.github.io/failprint","title":"failprint"},{"location":"showcase/failprint/#failprint","text":"Repository: https://github.com/pawamoy/failprint Documentation: https://pawamoy.github.io/failprint","title":"failprint"},{"location":"showcase/git-changelog/","text":"git-changelog \u00a4 Repository: https://github.com/pawamoy/git-changelog Documentation: https://pawamoy.github.io/git-changelog","title":"git-changelog"},{"location":"showcase/git-changelog/#git-changelog","text":"Repository: https://github.com/pawamoy/git-changelog Documentation: https://pawamoy.github.io/git-changelog","title":"git-changelog"},{"location":"showcase/mkdocstrings/","text":"mkdocstrings \u00a4 Repository: https://github.com/pawamoy/mkdocstrings Documentation: https://pawamoy.github.io/mkdocstrings","title":"mkdocstrings"},{"location":"showcase/mkdocstrings/#mkdocstrings","text":"Repository: https://github.com/pawamoy/mkdocstrings Documentation: https://pawamoy.github.io/mkdocstrings","title":"mkdocstrings"},{"location":"showcase/mvodb/","text":"mvodb \u00a4 Repository: https://github.com/pawamoy/mvodb","title":"mvodb"},{"location":"showcase/mvodb/#mvodb","text":"Repository: https://github.com/pawamoy/mvodb","title":"mvodb"},{"location":"showcase/pytkdocs/","text":"pytkdocs \u00a4 Repository: https://github.com/pawamoy/pytkdocs Documentation: https://pawamoy.github.io/pytkdocs","title":"pytkdocs"},{"location":"showcase/pytkdocs/#pytkdocs","text":"Repository: https://github.com/pawamoy/pytkdocs Documentation: https://pawamoy.github.io/pytkdocs","title":"pytkdocs"},{"location":"showcase/shell-history/","text":"shell-history \u00a4 Repository: https://github.com/pawamoy/shell-history","title":"shell-history"},{"location":"showcase/shell-history/#shell-history","text":"Repository: https://github.com/pawamoy/shell-history","title":"shell-history"},{"location":"showcase/shelldemo/","text":"shelldemo \u00a4 Repository: https://github.com/pawamoy/shelldemo","title":"shelldemo"},{"location":"showcase/shelldemo/#shelldemo","text":"Repository: https://github.com/pawamoy/shelldemo","title":"shelldemo"},{"location":"showcase/shellman/","text":"shellman \u00a4 Repository: https://github.com/pawamoy/shellman","title":"shellman"},{"location":"showcase/shellman/#shellman","text":"Repository: https://github.com/pawamoy/shellman","title":"shellman"}]}